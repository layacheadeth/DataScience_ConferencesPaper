{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c79c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0                                Planetwise Flannel Wipes   \n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "183526  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183527  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183528   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "183529  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "183530  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \n",
       "0       These flannel wipes are OK, but in my opinion ...       3  \n",
       "1       it came early and was not disappointed. i love...       5  \n",
       "2       Very soft and comfortable and warmer than it l...       5  \n",
       "3       This is a product well worth the purchase.  I ...       5  \n",
       "4       All of my kids have cried non-stop when I trie...       5  \n",
       "...                                                   ...     ...  \n",
       "183526  Such a great idea! very handy to have and look...       5  \n",
       "183527  This product rocks!  It is a great blend of fu...       5  \n",
       "183528  This item looks great and cool for my kids.......       5  \n",
       "183529  I am extremely happy with this product. I have...       5  \n",
       "183530  I love this product very mush . I have bought ...       5  \n",
       "\n",
       "[183531 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"amazon_baby.csv\",error_bad_lines=False)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4e1d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pystemmer in /Users/deth/opt/anaconda3/lib/python3.8/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Pystemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c32bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-d1db491ca8c1>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index']=data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text=data[[\"review\"]]\n",
    "data_text['index']=data_text.index\n",
    "\n",
    "documents=data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4821506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>183526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>183527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>183528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>183529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>183530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review   index\n",
       "0       These flannel wipes are OK, but in my opinion ...       0\n",
       "1       it came early and was not disappointed. i love...       1\n",
       "2       Very soft and comfortable and warmer than it l...       2\n",
       "3       This is a product well worth the purchase.  I ...       3\n",
       "4       All of my kids have cried non-stop when I trie...       4\n",
       "...                                                   ...     ...\n",
       "183526  Such a great idea! very handy to have and look...  183526\n",
       "183527  This product rocks!  It is a great blend of fu...  183527\n",
       "183528  This item looks great and cool for my kids.......  183528\n",
       "183529  I am extremely happy with this product. I have...  183529\n",
       "183530  I love this product very mush . I have bought ...  183530\n",
       "\n",
       "[183531 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b26d9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk.stem as stemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db41ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "      return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >3:\n",
    "              result.append(lemmatize_stemming(token))\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69f95aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document\n",
      "['They', 'were', 'for', 'my', 'daughter', '&', 'she', 'had', 'some', 'shells', 'like', 'this', 'before', 'and', 'liked', 'them.', 'My', 'sister', 'started', 'with', 'some', 'of', 'these', '40', 'years', 'ago', 'with', 'her', 'oldest', 'daughter', '-', 'got', 'me', 'some', 'for', 'me', 'when', 'my', 'daughter', 'was', 'born', '30', 'years', 'ago', '-', 'then', 'now', 'my', 'daughter', 'is', 'using', 'these', '-', 'we', 'are', 'great', 'fans.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['daughter', 'shell', 'like', 'like', 'sister', 'start', 'year', 'oldest', 'daughter', 'daughter', 'bear', 'year', 'daughter', 'great', 'fan']\n"
     ]
    }
   ],
   "source": [
    "doc_sample=documents[documents['index']==4310].values[0][0]\n",
    "print('original document')\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "      words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eddb90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-00cd468fc166>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
     ]
    }
   ],
   "source": [
    "documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7024f48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [flannel, wipe, opinion, worth, keep, order, s...\n",
       "1    [come, earli, disappoint, love, planet, wise, ...\n",
       "2    [soft, comfort, warmer, look, size, perfectli,...\n",
       "3    [product, worth, purchas, like, posit, ingeni,...\n",
       "4    [kid, cri, stop, tri, ween, pacifi, thumbuddi,...\n",
       "5    [binki, fairi, come, hous, special, gift, book...\n",
       "6    [love, book, bind, tightli, abl, alot, photo, ...\n",
       "7    [perfect, parent, abl, track, babi, feed, slee...\n",
       "8    [friend, pin, product, pinterest, decid, whirl...\n",
       "9    [easi, nanni, record, event, happen, babi, hom...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['review'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1f31839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blue\n",
      "1 boyfor\n",
      "2 cloth\n",
      "3 countwhich\n",
      "4 face\n",
      "5 flannel\n",
      "6 hand\n",
      "7 handl\n",
      "8 higher\n",
      "9 issu\n",
      "10 keep\n"
     ]
    }
   ],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)\n",
    "count=0\n",
    "\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd743c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.5,keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "533c4046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (4, 1),\n",
       " (36, 3),\n",
       " (60, 1),\n",
       " (64, 1),\n",
       " (75, 1),\n",
       " (177, 1),\n",
       " (283, 1),\n",
       " (288, 1),\n",
       " (384, 1),\n",
       " (418, 1),\n",
       " (541, 1),\n",
       " (655, 1),\n",
       " (684, 1),\n",
       " (696, 1),\n",
       " (705, 1),\n",
       " (2417, 1),\n",
       " (2427, 1),\n",
       " (2472, 1),\n",
       " (4219, 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bb9c105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"face\") appears 1 time.\n",
      "Word 4 (\"hand\") appears 1 time.\n",
      "Word 36 (\"love\") appears 3 time.\n",
      "Word 60 (\"littl\") appears 1 time.\n",
      "Word 64 (\"product\") appears 1 time.\n",
      "Word 75 (\"great\") appears 1 time.\n",
      "Word 177 (\"older\") appears 1 time.\n",
      "Word 283 (\"turn\") appears 1 time.\n",
      "Word 288 (\"enjoy\") appears 1 time.\n",
      "Word 384 (\"carri\") appears 1 time.\n",
      "Word 418 (\"infant\") appears 1 time.\n",
      "Word 541 (\"stuff\") appears 1 time.\n",
      "Word 655 (\"free\") appears 1 time.\n",
      "Word 684 (\"wear\") appears 1 time.\n",
      "Word 696 (\"watch\") appears 1 time.\n",
      "Word 705 (\"cook\") appears 1 time.\n",
      "Word 2417 (\"stave\") appears 1 time.\n",
      "Word 2427 (\"oven\") appears 1 time.\n",
      "Word 2472 (\"dinner\") appears 1 time.\n",
      "Word 4219 (\"cuddl\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310=bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b0eb474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.15091501593165862),\n",
      " (1, 0.23393853569876197),\n",
      " (2, 0.1255369345636988),\n",
      " (3, 0.22598623908509632),\n",
      " (4, 0.09893873572438135),\n",
      " (5, 0.12114001415352099),\n",
      " (6, 0.16991794106277847),\n",
      " (7, 0.11531090952299693),\n",
      " (8, 0.10275819221163739),\n",
      " (9, 0.14398977201310792),\n",
      " (10, 0.12706961807603134),\n",
      " (11, 0.058789838547190325),\n",
      " (12, 0.07374427507753085),\n",
      " (13, 0.1975073447434405),\n",
      " (14, 0.222002426625964),\n",
      " (15, 0.1738036264342695),\n",
      " (16, 0.0987763115139214),\n",
      " (17, 0.12337834464728409),\n",
      " (18, 0.09206114474407778),\n",
      " (19, 0.12067577899359858),\n",
      " (20, 0.18354165227218894),\n",
      " (21, 0.17707136078831798),\n",
      " (22, 0.09944349619585185),\n",
      " (23, 0.2121994764612386),\n",
      " (24, 0.19151610875759867),\n",
      " (25, 0.17078868919605977),\n",
      " (26, 0.29896491821974913),\n",
      " (27, 0.5293714406926228),\n",
      " (28, 0.10374295805010908)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf=tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "  pprint(doc)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81eca70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.LdaMulticore(bow_corpus,num_topics=10,id2word=dictionary,passes=2,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "654ac278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0\n",
      "words:0.020*\"easi\" + 0.018*\"love\" + 0.017*\"great\" + 0.016*\"like\" + 0.016*\"clean\" + 0.015*\"food\" + 0.015*\"diaper\" + 0.015*\"babi\" + 0.014*\"chair\" + 0.012*\"littl\"\n",
      "Topic:1\n",
      "words:0.034*\"babi\" + 0.021*\"soft\" + 0.017*\"wash\" + 0.017*\"sleep\" + 0.015*\"blanket\" + 0.015*\"like\" + 0.015*\"cover\" + 0.012*\"size\" + 0.012*\"mattress\" + 0.012*\"crib\"\n",
      "Topic:2\n",
      "words:0.050*\"babi\" + 0.037*\"love\" + 0.022*\"month\" + 0.015*\"play\" + 0.015*\"like\" + 0.014*\"littl\" + 0.012*\"time\" + 0.012*\"gift\" + 0.011*\"daughter\" + 0.011*\"great\"\n",
      "Topic:3\n",
      "words:0.025*\"diaper\" + 0.021*\"product\" + 0.015*\"work\" + 0.012*\"time\" + 0.010*\"review\" + 0.010*\"order\" + 0.010*\"return\" + 0.009*\"cloth\" + 0.008*\"money\" + 0.008*\"amazon\"\n",
      "Topic:4\n",
      "words:0.042*\"love\" + 0.022*\"great\" + 0.020*\"daughter\" + 0.020*\"year\" + 0.019*\"easi\" + 0.014*\"littl\" + 0.013*\"month\" + 0.013*\"buy\" + 0.012*\"recommend\" + 0.011*\"like\"\n",
      "Topic:5\n",
      "words:0.045*\"stroller\" + 0.026*\"monitor\" + 0.021*\"babi\" + 0.011*\"camera\" + 0.011*\"great\" + 0.010*\"work\" + 0.009*\"like\" + 0.009*\"carrier\" + 0.009*\"fold\" + 0.008*\"easi\"\n",
      "Topic:6\n",
      "words:0.025*\"open\" + 0.015*\"work\" + 0.015*\"gate\" + 0.011*\"easi\" + 0.010*\"smell\" + 0.010*\"door\" + 0.009*\"close\" + 0.009*\"wall\" + 0.009*\"like\" + 0.009*\"need\"\n",
      "Topic:7\n",
      "words:0.032*\"look\" + 0.031*\"color\" + 0.028*\"love\" + 0.024*\"great\" + 0.022*\"qualiti\" + 0.018*\"good\" + 0.018*\"cute\" + 0.017*\"like\" + 0.017*\"price\" + 0.015*\"nice\"\n",
      "Topic:8\n",
      "words:0.047*\"bottl\" + 0.024*\"pump\" + 0.014*\"like\" + 0.014*\"work\" + 0.013*\"nippl\" + 0.013*\"cup\" + 0.013*\"babi\" + 0.013*\"water\" + 0.012*\"leak\" + 0.011*\"milk\"\n",
      "Topic:9\n",
      "words:0.094*\"seat\" + 0.018*\"strap\" + 0.013*\"instal\" + 0.011*\"like\" + 0.011*\"easi\" + 0.011*\"face\" + 0.010*\"adjust\" + 0.009*\"child\" + 0.009*\"britax\" + 0.008*\"carseat\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "      print('Topic:{}\\nwords:{}'.format(idx,topic))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18e245d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.013*\"mattress\" + 0.012*\"blanket\" + 0.011*\"sheet\" + 0.009*\"crib\" + 0.009*\"sleep\" + 0.009*\"soft\" + 0.007*\"babi\" + 0.006*\"swaddl\" + 0.006*\"wash\" + 0.005*\"fit\"\n",
      "Topic: 1 Word: 0.029*\"diaper\" + 0.014*\"cloth\" + 0.010*\"wipe\" + 0.008*\"wash\" + 0.007*\"chang\" + 0.007*\"smell\" + 0.006*\"pocket\" + 0.006*\"absorb\" + 0.005*\"great\" + 0.005*\"insert\"\n",
      "Topic: 2 Word: 0.028*\"bottl\" + 0.021*\"pump\" + 0.013*\"nippl\" + 0.011*\"pacifi\" + 0.008*\"milk\" + 0.007*\"breast\" + 0.007*\"work\" + 0.007*\"medela\" + 0.007*\"spoon\" + 0.007*\"babi\"\n",
      "Topic: 3 Word: 0.021*\"monitor\" + 0.010*\"batteri\" + 0.009*\"sound\" + 0.008*\"camera\" + 0.008*\"work\" + 0.007*\"night\" + 0.007*\"swing\" + 0.007*\"light\" + 0.006*\"babi\" + 0.006*\"unit\"\n",
      "Topic: 4 Word: 0.027*\"seat\" + 0.012*\"instal\" + 0.010*\"gate\" + 0.008*\"easi\" + 0.006*\"britax\" + 0.006*\"open\" + 0.006*\"strap\" + 0.006*\"door\" + 0.005*\"carseat\" + 0.005*\"work\"\n",
      "Topic: 5 Word: 0.016*\"love\" + 0.012*\"play\" + 0.011*\"babi\" + 0.010*\"toy\" + 0.010*\"month\" + 0.008*\"daughter\" + 0.007*\"littl\" + 0.006*\"great\" + 0.006*\"like\" + 0.006*\"chew\"\n",
      "Topic: 6 Word: 0.011*\"cup\" + 0.009*\"food\" + 0.009*\"leak\" + 0.008*\"water\" + 0.008*\"clean\" + 0.008*\"sippi\" + 0.008*\"straw\" + 0.006*\"bottl\" + 0.006*\"drink\" + 0.006*\"love\"\n",
      "Topic: 7 Word: 0.013*\"pillow\" + 0.011*\"carrier\" + 0.010*\"babi\" + 0.007*\"head\" + 0.007*\"comfort\" + 0.006*\"bib\" + 0.006*\"sleep\" + 0.006*\"love\" + 0.006*\"support\" + 0.005*\"like\"\n",
      "Topic: 8 Word: 0.026*\"stroller\" + 0.013*\"chair\" + 0.011*\"seat\" + 0.010*\"easi\" + 0.008*\"fold\" + 0.007*\"love\" + 0.006*\"tray\" + 0.006*\"great\" + 0.005*\"high\" + 0.005*\"wheel\"\n",
      "Topic: 9 Word: 0.013*\"cute\" + 0.012*\"color\" + 0.012*\"look\" + 0.012*\"love\" + 0.011*\"qualiti\" + 0.009*\"great\" + 0.009*\"gift\" + 0.009*\"good\" + 0.008*\"nice\" + 0.008*\"babi\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf=gensim.models.LdaMulticore(corpus_tfidf,num_topics=10,id2word=dictionary,passes=2,workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9520a9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter',\n",
       " 'shell',\n",
       " 'like',\n",
       " 'like',\n",
       " 'sister',\n",
       " 'start',\n",
       " 'year',\n",
       " 'oldest',\n",
       " 'daughter',\n",
       " 'daughter',\n",
       " 'bear',\n",
       " 'year',\n",
       " 'daughter',\n",
       " 'great',\n",
       " 'fan']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07d4a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.8341153860092163\t \n",
      "Topic:0.050*\"babi\" + 0.037*\"love\" + 0.022*\"month\" + 0.015*\"play\" + 0.015*\"like\" + 0.014*\"littl\" + 0.012*\"time\" + 0.012*\"gift\" + 0.011*\"daughter\" + 0.011*\"great\"\n",
      "\n",
      "Score:0.13108089566230774\t \n",
      "Topic:0.025*\"open\" + 0.015*\"work\" + 0.015*\"gate\" + 0.011*\"easi\" + 0.010*\"smell\" + 0.010*\"door\" + 0.009*\"close\" + 0.009*\"wall\" + 0.009*\"like\" + 0.009*\"need\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lda_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ccafe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi_model=LsiModel(bow_corpus,num_topics=10,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b11cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0\n",
      "words:0.377*\"babi\" + 0.347*\"seat\" + 0.243*\"like\" + 0.205*\"love\" + 0.199*\"stroller\" + 0.164*\"great\" + 0.161*\"time\" + 0.149*\"month\" + 0.138*\"easi\" + 0.137*\"littl\"\n",
      "Topic:1\n",
      "words:-0.801*\"seat\" + 0.328*\"babi\" + -0.196*\"stroller\" + 0.150*\"bottl\" + 0.115*\"diaper\" + -0.084*\"strap\" + 0.083*\"love\" + 0.083*\"pump\" + 0.082*\"work\" + 0.080*\"monitor\"\n",
      "Topic:2\n",
      "words:0.688*\"stroller\" + -0.515*\"babi\" + -0.303*\"seat\" + 0.163*\"diaper\" + 0.104*\"fold\" + 0.086*\"wheel\" + 0.075*\"great\" + -0.066*\"chair\" + 0.064*\"handl\" + -0.061*\"carrier\"\n",
      "Topic:3\n",
      "words:-0.529*\"babi\" + -0.450*\"stroller\" + 0.421*\"bottl\" + 0.328*\"pump\" + 0.154*\"work\" + 0.116*\"nippl\" + 0.114*\"seat\" + 0.111*\"milk\" + 0.098*\"time\" + 0.093*\"clean\"\n",
      "Topic:4\n",
      "words:-0.547*\"bottl\" + 0.347*\"diaper\" + 0.333*\"love\" + -0.312*\"stroller\" + -0.304*\"babi\" + -0.257*\"pump\" + -0.159*\"nippl\" + -0.107*\"milk\" + 0.100*\"littl\" + 0.092*\"great\"\n",
      "Topic:5\n",
      "words:-0.736*\"diaper\" + 0.372*\"love\" + -0.172*\"cloth\" + -0.164*\"babi\" + 0.140*\"monitor\" + -0.128*\"bottl\" + 0.102*\"great\" + -0.095*\"cover\" + -0.090*\"seat\" + 0.087*\"daughter\"\n",
      "Topic:6\n",
      "words:0.648*\"love\" + 0.360*\"bottl\" + -0.321*\"monitor\" + -0.203*\"work\" + -0.155*\"pump\" + -0.114*\"camera\" + -0.114*\"product\" + -0.107*\"crib\" + -0.104*\"like\" + 0.096*\"nippl\"\n",
      "Topic:7\n",
      "words:-0.742*\"pump\" + 0.352*\"bottl\" + 0.230*\"like\" + -0.214*\"love\" + -0.154*\"diaper\" + -0.135*\"medela\" + -0.118*\"babi\" + 0.111*\"gate\" + -0.100*\"milk\" + 0.090*\"nippl\"\n",
      "Topic:8\n",
      "words:-0.619*\"gate\" + 0.441*\"monitor\" + -0.209*\"open\" + -0.171*\"easi\" + 0.169*\"camera\" + 0.155*\"bottl\" + 0.135*\"diaper\" + 0.135*\"love\" + 0.131*\"night\" + -0.115*\"instal\"\n",
      "Topic:9\n",
      "words:-0.527*\"pillow\" + 0.372*\"great\" + -0.350*\"sleep\" + 0.264*\"chair\" + -0.210*\"like\" + 0.199*\"easi\" + 0.191*\"monitor\" + -0.140*\"night\" + 0.120*\"clean\" + 0.110*\"product\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lsi_model.print_topics(-1):\n",
    "      print('Topic:{}\\nwords:{}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43c61146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.197*\"seat\" + 0.190*\"love\" + 0.188*\"babi\" + 0.166*\"great\" + 0.146*\"easi\" + 0.143*\"like\" + 0.129*\"month\" + 0.129*\"stroller\" + 0.127*\"product\" + 0.123*\"littl\"\n",
      "Topic: 1 Word: -0.645*\"seat\" + 0.400*\"bottl\" + -0.285*\"stroller\" + 0.175*\"pump\" + 0.136*\"nippl\" + -0.103*\"chair\" + 0.102*\"diaper\" + -0.098*\"instal\" + -0.095*\"strap\" + 0.089*\"leak\"\n",
      "Topic: 2 Word: -0.597*\"bottl\" + -0.292*\"seat\" + -0.268*\"pump\" + -0.195*\"nippl\" + 0.159*\"crib\" + -0.135*\"stroller\" + 0.128*\"monitor\" + 0.126*\"love\" + 0.122*\"sleep\" + 0.114*\"soft\"\n",
      "Topic: 3 Word: -0.531*\"stroller\" + -0.485*\"diaper\" + 0.268*\"seat\" + -0.169*\"cloth\" + 0.165*\"monitor\" + 0.138*\"swing\" + -0.102*\"fold\" + -0.095*\"chang\" + 0.095*\"sleep\" + 0.094*\"play\"\n",
      "Topic: 4 Word: 0.583*\"stroller\" + -0.453*\"diaper\" + -0.212*\"seat\" + -0.207*\"chair\" + -0.176*\"cloth\" + -0.158*\"potti\" + -0.118*\"wash\" + -0.116*\"clean\" + 0.107*\"swing\" + -0.107*\"chang\"\n",
      "Topic: 5 Word: -0.740*\"gate\" + -0.207*\"open\" + 0.170*\"bottl\" + -0.148*\"instal\" + 0.146*\"seat\" + 0.144*\"love\" + -0.134*\"pump\" + -0.133*\"door\" + -0.121*\"work\" + 0.113*\"soft\"\n",
      "Topic: 6 Word: -0.724*\"pump\" + 0.330*\"bottl\" + 0.201*\"gate\" + -0.150*\"diaper\" + 0.133*\"chair\" + 0.127*\"love\" + -0.118*\"pillow\" + -0.117*\"medela\" + -0.116*\"work\" + 0.115*\"easi\"\n",
      "Topic: 7 Word: 0.384*\"crib\" + -0.343*\"swing\" + 0.295*\"mattress\" + -0.257*\"diaper\" + 0.249*\"sheet\" + 0.176*\"qualiti\" + 0.160*\"good\" + -0.160*\"love\" + 0.149*\"pump\" + -0.132*\"music\"\n",
      "Topic: 8 Word: 0.438*\"chair\" + -0.407*\"pillow\" + -0.297*\"gate\" + -0.203*\"sleep\" + -0.195*\"blanket\" + -0.194*\"seat\" + 0.186*\"pump\" + 0.164*\"easi\" + 0.144*\"clean\" + 0.140*\"tray\"\n",
      "Topic: 9 Word: -0.498*\"pillow\" + -0.416*\"chair\" + -0.251*\"sleep\" + 0.228*\"cute\" + 0.222*\"seat\" + 0.130*\"pump\" + -0.128*\"tray\" + 0.128*\"gift\" + 0.126*\"qualiti\" + -0.124*\"night\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model_tfidf=LsiModel(corpus_tfidf,num_topics=10,id2word=dictionary)\n",
    "for idx, topic in lsi_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de325115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter',\n",
       " 'shell',\n",
       " 'like',\n",
       " 'like',\n",
       " 'sister',\n",
       " 'start',\n",
       " 'year',\n",
       " 'oldest',\n",
       " 'daughter',\n",
       " 'daughter',\n",
       " 'bear',\n",
       " 'year',\n",
       " 'daughter',\n",
       " 'great',\n",
       " 'fan']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b764dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:1.8143148663294555\t \n",
      "Topic:0.648*\"love\" + 0.360*\"bottl\" + -0.321*\"monitor\" + -0.203*\"work\" + -0.155*\"pump\" + -0.114*\"camera\" + -0.114*\"product\" + -0.107*\"crib\" + -0.104*\"like\" + 0.096*\"nippl\"\n",
      "\n",
      "Score:1.3612839389968816\t \n",
      "Topic:-0.736*\"diaper\" + 0.372*\"love\" + -0.172*\"cloth\" + -0.164*\"babi\" + 0.140*\"monitor\" + -0.128*\"bottl\" + 0.102*\"great\" + -0.095*\"cover\" + -0.090*\"seat\" + 0.087*\"daughter\"\n",
      "\n",
      "Score:1.357350846479559\t \n",
      "Topic:0.377*\"babi\" + 0.347*\"seat\" + 0.243*\"like\" + 0.205*\"love\" + 0.199*\"stroller\" + 0.164*\"great\" + 0.161*\"time\" + 0.149*\"month\" + 0.138*\"easi\" + 0.137*\"littl\"\n",
      "\n",
      "Score:1.2222560569850827\t \n",
      "Topic:-0.547*\"bottl\" + 0.347*\"diaper\" + 0.333*\"love\" + -0.312*\"stroller\" + -0.304*\"babi\" + -0.257*\"pump\" + -0.159*\"nippl\" + -0.107*\"milk\" + 0.100*\"littl\" + 0.092*\"great\"\n",
      "\n",
      "Score:0.44860757645832894\t \n",
      "Topic:-0.801*\"seat\" + 0.328*\"babi\" + -0.196*\"stroller\" + 0.150*\"bottl\" + 0.115*\"diaper\" + -0.084*\"strap\" + 0.083*\"love\" + 0.083*\"pump\" + 0.082*\"work\" + 0.080*\"monitor\"\n",
      "\n",
      "Score:0.35727699428737\t \n",
      "Topic:-0.619*\"gate\" + 0.441*\"monitor\" + -0.209*\"open\" + -0.171*\"easi\" + 0.169*\"camera\" + 0.155*\"bottl\" + 0.135*\"diaper\" + 0.135*\"love\" + 0.131*\"night\" + -0.115*\"instal\"\n",
      "\n",
      "Score:0.3567745624481061\t \n",
      "Topic:-0.527*\"pillow\" + 0.372*\"great\" + -0.350*\"sleep\" + 0.264*\"chair\" + -0.210*\"like\" + 0.199*\"easi\" + 0.191*\"monitor\" + -0.140*\"night\" + 0.120*\"clean\" + 0.110*\"product\"\n",
      "\n",
      "Score:0.24668801229639703\t \n",
      "Topic:0.688*\"stroller\" + -0.515*\"babi\" + -0.303*\"seat\" + 0.163*\"diaper\" + 0.104*\"fold\" + 0.086*\"wheel\" + 0.075*\"great\" + -0.066*\"chair\" + 0.064*\"handl\" + -0.061*\"carrier\"\n",
      "\n",
      "Score:0.1544204414584918\t \n",
      "Topic:-0.529*\"babi\" + -0.450*\"stroller\" + 0.421*\"bottl\" + 0.328*\"pump\" + 0.154*\"work\" + 0.116*\"nippl\" + 0.114*\"seat\" + 0.111*\"milk\" + 0.098*\"time\" + 0.093*\"clean\"\n",
      "\n",
      "Score:-0.5344273604502523\t \n",
      "Topic:-0.742*\"pump\" + 0.352*\"bottl\" + 0.230*\"like\" + -0.214*\"love\" + -0.154*\"diaper\" + -0.135*\"medela\" + -0.118*\"babi\" + 0.111*\"gate\" + -0.100*\"milk\" + 0.090*\"nippl\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lsi_model[bow_corpus[4310]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lsi_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "021daeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7748525738716125\t Topic: 0.025*\"diaper\" + 0.021*\"product\" + 0.015*\"work\" + 0.012*\"time\" + 0.010*\"review\"\n",
      "Score: 0.02501954697072506\t Topic: 0.094*\"seat\" + 0.018*\"strap\" + 0.013*\"instal\" + 0.011*\"like\" + 0.011*\"easi\"\n",
      "Score: 0.02501855418086052\t Topic: 0.032*\"look\" + 0.031*\"color\" + 0.028*\"love\" + 0.024*\"great\" + 0.022*\"qualiti\"\n",
      "Score: 0.025016961619257927\t Topic: 0.047*\"bottl\" + 0.024*\"pump\" + 0.014*\"like\" + 0.014*\"work\" + 0.013*\"nippl\"\n",
      "Score: 0.025016695261001587\t Topic: 0.045*\"stroller\" + 0.026*\"monitor\" + 0.021*\"babi\" + 0.011*\"camera\" + 0.011*\"great\"\n",
      "Score: 0.025015519931912422\t Topic: 0.025*\"open\" + 0.015*\"work\" + 0.015*\"gate\" + 0.011*\"easi\" + 0.010*\"smell\"\n",
      "Score: 0.025015370920300484\t Topic: 0.020*\"easi\" + 0.018*\"love\" + 0.017*\"great\" + 0.016*\"like\" + 0.016*\"clean\"\n",
      "Score: 0.02501516416668892\t Topic: 0.034*\"babi\" + 0.021*\"soft\" + 0.017*\"wash\" + 0.017*\"sleep\" + 0.015*\"blanket\"\n",
      "Score: 0.02501486800611019\t Topic: 0.042*\"love\" + 0.022*\"great\" + 0.020*\"daughter\" + 0.020*\"year\" + 0.019*\"easi\"\n",
      "Score: 0.025014782324433327\t Topic: 0.050*\"babi\" + 0.037*\"love\" + 0.022*\"month\" + 0.015*\"play\" + 0.015*\"like\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51160985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.019396821168481973\t Topic: 0.020*\"easi\" + 0.018*\"love\" + 0.017*\"great\" + 0.016*\"like\" + 0.016*\"clean\"\n",
      "Score: 0.007938200150906394\t Topic: 0.032*\"look\" + 0.031*\"color\" + 0.028*\"love\" + 0.024*\"great\" + 0.022*\"qualiti\"\n",
      "Score: 0.007539470341212645\t Topic: 0.050*\"babi\" + 0.037*\"love\" + 0.022*\"month\" + 0.015*\"play\" + 0.015*\"like\"\n",
      "Score: 0.005707382134600589\t Topic: 0.025*\"diaper\" + 0.021*\"product\" + 0.015*\"work\" + 0.012*\"time\" + 0.010*\"review\"\n",
      "Score: 0.004477394925486246\t Topic: 0.047*\"bottl\" + 0.024*\"pump\" + 0.014*\"like\" + 0.014*\"work\" + 0.013*\"nippl\"\n",
      "Score: 0.0009141136161267432\t Topic: 0.094*\"seat\" + 0.018*\"strap\" + 0.013*\"instal\" + 0.011*\"like\" + 0.011*\"easi\"\n",
      "Score: 0.00026212046505711783\t Topic: 0.034*\"babi\" + 0.021*\"soft\" + 0.017*\"wash\" + 0.017*\"sleep\" + 0.015*\"blanket\"\n",
      "Score: -0.0001826484776736863\t Topic: 0.042*\"love\" + 0.022*\"great\" + 0.020*\"daughter\" + 0.020*\"year\" + 0.019*\"easi\"\n",
      "Score: -0.0007125234863536439\t Topic: 0.045*\"stroller\" + 0.026*\"monitor\" + 0.021*\"babi\" + 0.011*\"camera\" + 0.011*\"great\"\n",
      "Score: -0.010672477433712618\t Topic: 0.025*\"open\" + 0.015*\"work\" + 0.015*\"gate\" + 0.011*\"easi\" + 0.010*\"smell\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lsi_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0e4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6477534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe234811",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
