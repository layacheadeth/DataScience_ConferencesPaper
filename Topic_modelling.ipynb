{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c611b814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0                                Planetwise Flannel Wipes   \n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "183526  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183527  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183528   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "183529  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "183530  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \n",
       "0       These flannel wipes are OK, but in my opinion ...       3  \n",
       "1       it came early and was not disappointed. i love...       5  \n",
       "2       Very soft and comfortable and warmer than it l...       5  \n",
       "3       This is a product well worth the purchase.  I ...       5  \n",
       "4       All of my kids have cried non-stop when I trie...       5  \n",
       "...                                                   ...     ...  \n",
       "183526  Such a great idea! very handy to have and look...       5  \n",
       "183527  This product rocks!  It is a great blend of fu...       5  \n",
       "183528  This item looks great and cool for my kids.......       5  \n",
       "183529  I am extremely happy with this product. I have...       5  \n",
       "183530  I love this product very mush . I have bought ...       5  \n",
       "\n",
       "[183531 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"amazon_baby.csv\",error_bad_lines=False)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa087c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pystemmer in /Users/deth/opt/anaconda3/lib/python3.8/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Pystemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d45917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4a6f3181a26a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index']=data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text=data[[\"review\"]]\n",
    "data_text['index']=data_text.index\n",
    "\n",
    "documents=data_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16e2a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>183526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>183527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>183528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>183529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>183530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review   index\n",
       "0       These flannel wipes are OK, but in my opinion ...       0\n",
       "1       it came early and was not disappointed. i love...       1\n",
       "2       Very soft and comfortable and warmer than it l...       2\n",
       "3       This is a product well worth the purchase.  I ...       3\n",
       "4       All of my kids have cried non-stop when I trie...       4\n",
       "...                                                   ...     ...\n",
       "183526  Such a great idea! very handy to have and look...  183526\n",
       "183527  This product rocks!  It is a great blend of fu...  183527\n",
       "183528  This item looks great and cool for my kids.......  183528\n",
       "183529  I am extremely happy with this product. I have...  183529\n",
       "183530  I love this product very mush . I have bought ...  183530\n",
       "\n",
       "[183531 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1d50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504383ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1c8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk.stem as stemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29394b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "      return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >3:\n",
    "              result.append(lemmatize_stemming(token))\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f082f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document\n",
      "['They', 'were', 'for', 'my', 'daughter', '&', 'she', 'had', 'some', 'shells', 'like', 'this', 'before', 'and', 'liked', 'them.', 'My', 'sister', 'started', 'with', 'some', 'of', 'these', '40', 'years', 'ago', 'with', 'her', 'oldest', 'daughter', '-', 'got', 'me', 'some', 'for', 'me', 'when', 'my', 'daughter', 'was', 'born', '30', 'years', 'ago', '-', 'then', 'now', 'my', 'daughter', 'is', 'using', 'these', '-', 'we', 'are', 'great', 'fans.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['daughter', 'shell', 'like', 'like', 'sister', 'start', 'year', 'oldest', 'daughter', 'daughter', 'bear', 'year', 'daughter', 'great', 'fan']\n"
     ]
    }
   ],
   "source": [
    "doc_sample=documents[documents['index']==4310].values[0][0]\n",
    "print('original document')\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "      words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6800a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_docs=documents['review'].map(preprocess)\n",
    "# processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d225a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-00cd468fc166>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
     ]
    }
   ],
   "source": [
    "documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40d8928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [flannel, wipe, opinion, worth, keep, order, s...\n",
       "1    [come, earli, disappoint, love, planet, wise, ...\n",
       "2    [soft, comfort, warmer, look, size, perfectli,...\n",
       "3    [product, worth, purchas, like, posit, ingeni,...\n",
       "4    [kid, cri, stop, tri, ween, pacifi, thumbuddi,...\n",
       "5    [binki, fairi, come, hous, special, gift, book...\n",
       "6    [love, book, bind, tightli, abl, alot, photo, ...\n",
       "7    [perfect, parent, abl, track, babi, feed, slee...\n",
       "8    [friend, pin, product, pinterest, decid, whirl...\n",
       "9    [easi, nanni, record, event, happen, babi, hom...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['review'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae1d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blue\n",
      "1 boyfor\n",
      "2 cloth\n",
      "3 countwhich\n",
      "4 face\n",
      "5 flannel\n",
      "6 hand\n",
      "7 handl\n",
      "8 higher\n",
      "9 issu\n",
      "10 keep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary=gensim.corpora.Dictionary(processed_docs)\n",
    "count=0\n",
    "\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee10b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.5,keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3413f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (4, 1),\n",
       " (36, 3),\n",
       " (60, 1),\n",
       " (64, 1),\n",
       " (75, 1),\n",
       " (177, 1),\n",
       " (283, 1),\n",
       " (288, 1),\n",
       " (384, 1),\n",
       " (418, 1),\n",
       " (541, 1),\n",
       " (655, 1),\n",
       " (684, 1),\n",
       " (696, 1),\n",
       " (705, 1),\n",
       " (2417, 1),\n",
       " (2427, 1),\n",
       " (2472, 1),\n",
       " (4219, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b091c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bf8b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"face\") appears 1 time.\n",
      "Word 4 (\"hand\") appears 1 time.\n",
      "Word 36 (\"love\") appears 3 time.\n",
      "Word 60 (\"littl\") appears 1 time.\n",
      "Word 64 (\"product\") appears 1 time.\n",
      "Word 75 (\"great\") appears 1 time.\n",
      "Word 177 (\"older\") appears 1 time.\n",
      "Word 283 (\"turn\") appears 1 time.\n",
      "Word 288 (\"enjoy\") appears 1 time.\n",
      "Word 384 (\"carri\") appears 1 time.\n",
      "Word 418 (\"infant\") appears 1 time.\n",
      "Word 541 (\"stuff\") appears 1 time.\n",
      "Word 655 (\"free\") appears 1 time.\n",
      "Word 684 (\"wear\") appears 1 time.\n",
      "Word 696 (\"watch\") appears 1 time.\n",
      "Word 705 (\"cook\") appears 1 time.\n",
      "Word 2417 (\"stave\") appears 1 time.\n",
      "Word 2427 (\"oven\") appears 1 time.\n",
      "Word 2472 (\"dinner\") appears 1 time.\n",
      "Word 4219 (\"cuddl\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310=bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1871e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbe21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac83ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4046d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf1ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1da6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6905940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a21ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7701a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618627a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c2b1eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.15091501593165862),\n",
      " (1, 0.23393853569876197),\n",
      " (2, 0.1255369345636988),\n",
      " (3, 0.22598623908509632),\n",
      " (4, 0.09893873572438135),\n",
      " (5, 0.12114001415352099),\n",
      " (6, 0.16991794106277847),\n",
      " (7, 0.11531090952299693),\n",
      " (8, 0.10275819221163739),\n",
      " (9, 0.14398977201310792),\n",
      " (10, 0.12706961807603134),\n",
      " (11, 0.058789838547190325),\n",
      " (12, 0.07374427507753085),\n",
      " (13, 0.1975073447434405),\n",
      " (14, 0.222002426625964),\n",
      " (15, 0.1738036264342695),\n",
      " (16, 0.0987763115139214),\n",
      " (17, 0.12337834464728409),\n",
      " (18, 0.09206114474407778),\n",
      " (19, 0.12067577899359858),\n",
      " (20, 0.18354165227218894),\n",
      " (21, 0.17707136078831798),\n",
      " (22, 0.09944349619585185),\n",
      " (23, 0.2121994764612386),\n",
      " (24, 0.19151610875759867),\n",
      " (25, 0.17078868919605977),\n",
      " (26, 0.29896491821974913),\n",
      " (27, 0.5293714406926228),\n",
      " (28, 0.10374295805010908)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf=tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "  pprint(doc)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180fba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.LdaMulticore(bow_corpus,num_topics=10,id2word=dictionary,passes=2,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67af6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0\n",
      "words:0.028*\"monitor\" + 0.017*\"work\" + 0.016*\"babi\" + 0.014*\"product\" + 0.012*\"camera\" + 0.009*\"time\" + 0.009*\"batteri\" + 0.008*\"night\" + 0.008*\"review\" + 0.008*\"unit\"\n",
      "Topic:1\n",
      "words:0.019*\"potti\" + 0.018*\"seat\" + 0.012*\"year\" + 0.012*\"littl\" + 0.012*\"like\" + 0.011*\"toilet\" + 0.010*\"daughter\" + 0.010*\"mirror\" + 0.010*\"train\" + 0.010*\"child\"\n",
      "Topic:2\n",
      "words:0.043*\"love\" + 0.033*\"babi\" + 0.029*\"play\" + 0.024*\"month\" + 0.018*\"like\" + 0.016*\"toy\" + 0.015*\"littl\" + 0.013*\"great\" + 0.012*\"daughter\" + 0.012*\"light\"\n",
      "Topic:3\n",
      "words:0.018*\"chair\" + 0.014*\"open\" + 0.013*\"easi\" + 0.011*\"work\" + 0.010*\"gate\" + 0.009*\"like\" + 0.008*\"piec\" + 0.008*\"instal\" + 0.008*\"need\" + 0.008*\"product\"\n",
      "Topic:4\n",
      "words:0.031*\"babi\" + 0.027*\"sleep\" + 0.025*\"pump\" + 0.017*\"time\" + 0.016*\"work\" + 0.014*\"pillow\" + 0.012*\"comfort\" + 0.012*\"night\" + 0.011*\"month\" + 0.010*\"help\"\n",
      "Topic:5\n",
      "words:0.024*\"babi\" + 0.020*\"love\" + 0.018*\"like\" + 0.017*\"food\" + 0.015*\"clean\" + 0.015*\"easi\" + 0.013*\"littl\" + 0.013*\"great\" + 0.012*\"cup\" + 0.011*\"water\"\n",
      "Topic:6\n",
      "words:0.040*\"diaper\" + 0.018*\"wash\" + 0.016*\"cover\" + 0.016*\"like\" + 0.015*\"cloth\" + 0.015*\"babi\" + 0.012*\"size\" + 0.012*\"great\" + 0.011*\"chang\" + 0.010*\"wipe\"\n",
      "Topic:7\n",
      "words:0.038*\"love\" + 0.027*\"great\" + 0.026*\"look\" + 0.022*\"color\" + 0.022*\"babi\" + 0.018*\"qualiti\" + 0.017*\"cute\" + 0.016*\"product\" + 0.015*\"good\" + 0.014*\"crib\"\n",
      "Topic:8\n",
      "words:0.078*\"bottl\" + 0.022*\"nippl\" + 0.021*\"mattress\" + 0.014*\"milk\" + 0.012*\"like\" + 0.011*\"babi\" + 0.010*\"leak\" + 0.010*\"work\" + 0.010*\"clean\" + 0.009*\"water\"\n",
      "Topic:9\n",
      "words:0.061*\"seat\" + 0.036*\"stroller\" + 0.016*\"easi\" + 0.014*\"strap\" + 0.013*\"love\" + 0.012*\"like\" + 0.010*\"babi\" + 0.010*\"great\" + 0.009*\"fold\" + 0.008*\"comfort\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "      print('Topic:{}\\nwords:{}'.format(idx,topic))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f8bf094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.017*\"chair\" + 0.014*\"food\" + 0.011*\"easi\" + 0.010*\"clean\" + 0.009*\"tray\" + 0.007*\"spoon\" + 0.007*\"great\" + 0.007*\"love\" + 0.007*\"tabl\" + 0.007*\"high\"\n",
      "Topic: 1 Word: 0.025*\"seat\" + 0.009*\"mattress\" + 0.008*\"comfort\" + 0.008*\"pillow\" + 0.007*\"crib\" + 0.007*\"sleep\" + 0.006*\"easi\" + 0.006*\"babi\" + 0.006*\"strap\" + 0.006*\"instal\"\n",
      "Topic: 2 Word: 0.012*\"color\" + 0.008*\"love\" + 0.008*\"pink\" + 0.007*\"cute\" + 0.007*\"look\" + 0.006*\"babi\" + 0.006*\"pictur\" + 0.005*\"like\" + 0.005*\"nice\" + 0.005*\"soft\"\n",
      "Topic: 3 Word: 0.021*\"diaper\" + 0.011*\"cloth\" + 0.011*\"wash\" + 0.009*\"blanket\" + 0.009*\"soft\" + 0.008*\"size\" + 0.007*\"babi\" + 0.007*\"wipe\" + 0.007*\"cover\" + 0.007*\"love\"\n",
      "Topic: 4 Word: 0.014*\"potti\" + 0.013*\"bath\" + 0.009*\"toilet\" + 0.007*\"love\" + 0.007*\"seat\" + 0.006*\"water\" + 0.006*\"littl\" + 0.006*\"train\" + 0.006*\"great\" + 0.005*\"daughter\"\n",
      "Topic: 5 Word: 0.020*\"monitor\" + 0.009*\"batteri\" + 0.008*\"camera\" + 0.008*\"night\" + 0.008*\"babi\" + 0.008*\"sound\" + 0.008*\"work\" + 0.006*\"carrier\" + 0.006*\"swing\" + 0.006*\"sleep\"\n",
      "Topic: 6 Word: 0.012*\"gate\" + 0.008*\"open\" + 0.007*\"door\" + 0.006*\"product\" + 0.006*\"wall\" + 0.006*\"work\" + 0.005*\"instal\" + 0.005*\"look\" + 0.005*\"easi\" + 0.004*\"lock\"\n",
      "Topic: 7 Word: 0.016*\"love\" + 0.011*\"babi\" + 0.010*\"gift\" + 0.010*\"cute\" + 0.009*\"great\" + 0.009*\"play\" + 0.008*\"daughter\" + 0.008*\"month\" + 0.007*\"color\" + 0.007*\"look\"\n",
      "Topic: 8 Word: 0.026*\"bottl\" + 0.016*\"pump\" + 0.011*\"nippl\" + 0.010*\"cup\" + 0.010*\"leak\" + 0.008*\"milk\" + 0.008*\"sippi\" + 0.007*\"straw\" + 0.007*\"work\" + 0.007*\"clean\"\n",
      "Topic: 9 Word: 0.035*\"stroller\" + 0.009*\"seat\" + 0.008*\"fold\" + 0.007*\"wheel\" + 0.006*\"easi\" + 0.005*\"pacifi\" + 0.005*\"great\" + 0.005*\"shade\" + 0.005*\"love\" + 0.005*\"handl\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf=gensim.models.LdaMulticore(corpus_tfidf,num_topics=10,id2word=dictionary,passes=2,workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f62de71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter',\n",
       " 'shell',\n",
       " 'like',\n",
       " 'like',\n",
       " 'sister',\n",
       " 'start',\n",
       " 'year',\n",
       " 'oldest',\n",
       " 'daughter',\n",
       " 'daughter',\n",
       " 'bear',\n",
       " 'year',\n",
       " 'daughter',\n",
       " 'great',\n",
       " 'fan']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bbf6943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.5021318793296814\t \n",
      "Topic:0.043*\"love\" + 0.033*\"babi\" + 0.029*\"play\" + 0.024*\"month\" + 0.018*\"like\" + 0.016*\"toy\" + 0.015*\"littl\" + 0.013*\"great\" + 0.012*\"daughter\" + 0.012*\"light\"\n",
      "\n",
      "Score:0.376325398683548\t \n",
      "Topic:0.024*\"babi\" + 0.020*\"love\" + 0.018*\"like\" + 0.017*\"food\" + 0.015*\"clean\" + 0.015*\"easi\" + 0.013*\"littl\" + 0.013*\"great\" + 0.012*\"cup\" + 0.011*\"water\"\n",
      "\n",
      "Score:0.09108871966600418\t \n",
      "Topic:0.018*\"chair\" + 0.014*\"open\" + 0.013*\"easi\" + 0.011*\"work\" + 0.010*\"gate\" + 0.009*\"like\" + 0.008*\"piec\" + 0.008*\"instal\" + 0.008*\"need\" + 0.008*\"product\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lda_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad6b25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi_model=LsiModel(bow_corpus,num_topics=10,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36a38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0\n",
      "words:0.377*\"babi\" + 0.347*\"seat\" + 0.243*\"like\" + 0.205*\"love\" + 0.199*\"stroller\" + 0.164*\"great\" + 0.161*\"time\" + 0.149*\"month\" + 0.138*\"easi\" + 0.137*\"littl\"\n",
      "Topic:1\n",
      "words:-0.801*\"seat\" + 0.328*\"babi\" + -0.196*\"stroller\" + 0.150*\"bottl\" + 0.115*\"diaper\" + -0.084*\"strap\" + 0.083*\"love\" + 0.083*\"pump\" + 0.082*\"work\" + 0.080*\"monitor\"\n",
      "Topic:2\n",
      "words:0.688*\"stroller\" + -0.514*\"babi\" + -0.303*\"seat\" + 0.163*\"diaper\" + 0.104*\"fold\" + 0.086*\"wheel\" + 0.075*\"great\" + -0.066*\"chair\" + 0.064*\"handl\" + -0.061*\"carrier\"\n",
      "Topic:3\n",
      "words:-0.529*\"babi\" + -0.450*\"stroller\" + 0.421*\"bottl\" + 0.328*\"pump\" + 0.154*\"work\" + 0.116*\"nippl\" + 0.113*\"seat\" + 0.111*\"milk\" + 0.099*\"time\" + 0.093*\"clean\"\n",
      "Topic:4\n",
      "words:-0.547*\"bottl\" + 0.347*\"diaper\" + 0.333*\"love\" + -0.312*\"stroller\" + -0.304*\"babi\" + -0.257*\"pump\" + -0.159*\"nippl\" + -0.107*\"milk\" + 0.100*\"littl\" + 0.092*\"great\"\n",
      "Topic:5\n",
      "words:-0.736*\"diaper\" + 0.372*\"love\" + -0.172*\"cloth\" + -0.164*\"babi\" + 0.139*\"monitor\" + -0.128*\"bottl\" + 0.102*\"great\" + -0.095*\"cover\" + -0.090*\"seat\" + 0.087*\"daughter\"\n",
      "Topic:6\n",
      "words:0.648*\"love\" + 0.361*\"bottl\" + -0.321*\"monitor\" + -0.203*\"work\" + -0.156*\"pump\" + -0.114*\"camera\" + -0.114*\"product\" + -0.106*\"crib\" + -0.104*\"like\" + 0.096*\"nippl\"\n",
      "Topic:7\n",
      "words:-0.742*\"pump\" + 0.352*\"bottl\" + 0.230*\"like\" + -0.215*\"love\" + -0.154*\"diaper\" + -0.135*\"medela\" + -0.118*\"babi\" + 0.111*\"gate\" + -0.100*\"milk\" + 0.090*\"nippl\"\n",
      "Topic:8\n",
      "words:-0.619*\"gate\" + 0.440*\"monitor\" + -0.209*\"open\" + -0.172*\"easi\" + 0.169*\"camera\" + 0.155*\"bottl\" + 0.135*\"diaper\" + 0.135*\"love\" + 0.132*\"night\" + -0.115*\"instal\"\n",
      "Topic:9\n",
      "words:-0.526*\"pillow\" + 0.374*\"great\" + -0.349*\"sleep\" + 0.265*\"chair\" + -0.210*\"like\" + 0.199*\"easi\" + 0.190*\"monitor\" + -0.141*\"night\" + 0.121*\"clean\" + 0.111*\"product\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lsi_model.print_topics(-1):\n",
    "      print('Topic:{}\\nwords:{}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eaccbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.198*\"seat\" + 0.190*\"love\" + 0.188*\"babi\" + 0.166*\"great\" + 0.146*\"easi\" + 0.143*\"like\" + 0.129*\"stroller\" + 0.129*\"month\" + 0.128*\"product\" + 0.123*\"littl\"\n",
      "Topic: 1 Word: -0.645*\"seat\" + 0.400*\"bottl\" + -0.284*\"stroller\" + 0.176*\"pump\" + 0.136*\"nippl\" + -0.102*\"chair\" + 0.102*\"diaper\" + -0.099*\"instal\" + -0.095*\"strap\" + 0.089*\"leak\"\n",
      "Topic: 2 Word: -0.597*\"bottl\" + -0.292*\"seat\" + -0.269*\"pump\" + -0.197*\"nippl\" + 0.160*\"crib\" + -0.136*\"stroller\" + 0.130*\"monitor\" + 0.123*\"love\" + 0.121*\"sleep\" + 0.114*\"soft\"\n",
      "Topic: 3 Word: -0.509*\"diaper\" + -0.488*\"stroller\" + 0.256*\"seat\" + -0.178*\"cloth\" + 0.178*\"monitor\" + 0.145*\"swing\" + 0.102*\"sleep\" + -0.102*\"chang\" + 0.100*\"pump\" + -0.098*\"wipe\"\n",
      "Topic: 4 Word: -0.613*\"stroller\" + 0.418*\"diaper\" + 0.236*\"seat\" + 0.212*\"chair\" + 0.163*\"cloth\" + 0.161*\"potti\" + 0.116*\"clean\" + 0.109*\"wash\" + -0.104*\"pump\" + -0.101*\"swing\"\n",
      "Topic: 5 Word: -0.745*\"gate\" + -0.206*\"open\" + 0.165*\"bottl\" + -0.151*\"instal\" + 0.143*\"love\" + 0.136*\"seat\" + -0.133*\"pump\" + -0.132*\"door\" + -0.118*\"work\" + 0.111*\"babi\"\n",
      "Topic: 6 Word: -0.714*\"pump\" + 0.323*\"bottl\" + 0.182*\"gate\" + -0.159*\"diaper\" + 0.158*\"chair\" + 0.140*\"love\" + 0.122*\"easi\" + -0.118*\"pillow\" + -0.118*\"work\" + -0.116*\"medela\"\n",
      "Topic: 7 Word: 0.388*\"crib\" + -0.334*\"swing\" + 0.288*\"mattress\" + -0.263*\"diaper\" + 0.239*\"sheet\" + 0.180*\"qualiti\" + 0.163*\"good\" + 0.153*\"chair\" + -0.149*\"love\" + 0.146*\"pump\"\n",
      "Topic: 8 Word: 0.422*\"chair\" + -0.393*\"pillow\" + -0.281*\"gate\" + 0.233*\"pump\" + -0.202*\"sleep\" + -0.190*\"seat\" + -0.186*\"blanket\" + 0.157*\"easi\" + -0.155*\"bottl\" + 0.136*\"clean\"\n",
      "Topic: 9 Word: -0.528*\"pillow\" + -0.427*\"chair\" + -0.244*\"sleep\" + 0.231*\"seat\" + 0.199*\"cute\" + 0.130*\"qualiti\" + -0.127*\"tray\" + -0.118*\"high\" + -0.111*\"tabl\" + 0.109*\"gift\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model_tfidf=LsiModel(corpus_tfidf,num_topics=10,id2word=dictionary)\n",
    "for idx, topic in lsi_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04d9e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter',\n",
       " 'shell',\n",
       " 'like',\n",
       " 'like',\n",
       " 'sister',\n",
       " 'start',\n",
       " 'year',\n",
       " 'oldest',\n",
       " 'daughter',\n",
       " 'daughter',\n",
       " 'bear',\n",
       " 'year',\n",
       " 'daughter',\n",
       " 'great',\n",
       " 'fan']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "480f6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:1.814780678101684\t \n",
      "Topic:0.648*\"love\" + 0.361*\"bottl\" + -0.321*\"monitor\" + -0.203*\"work\" + -0.156*\"pump\" + -0.114*\"camera\" + -0.114*\"product\" + -0.106*\"crib\" + -0.104*\"like\" + 0.096*\"nippl\"\n",
      "\n",
      "Score:1.3619103946945599\t \n",
      "Topic:-0.736*\"diaper\" + 0.372*\"love\" + -0.172*\"cloth\" + -0.164*\"babi\" + 0.139*\"monitor\" + -0.128*\"bottl\" + 0.102*\"great\" + -0.095*\"cover\" + -0.090*\"seat\" + 0.087*\"daughter\"\n",
      "\n",
      "Score:1.3573526065321442\t \n",
      "Topic:0.377*\"babi\" + 0.347*\"seat\" + 0.243*\"like\" + 0.205*\"love\" + 0.199*\"stroller\" + 0.164*\"great\" + 0.161*\"time\" + 0.149*\"month\" + 0.138*\"easi\" + 0.137*\"littl\"\n",
      "\n",
      "Score:1.2230213648465333\t \n",
      "Topic:-0.547*\"bottl\" + 0.347*\"diaper\" + 0.333*\"love\" + -0.312*\"stroller\" + -0.304*\"babi\" + -0.257*\"pump\" + -0.159*\"nippl\" + -0.107*\"milk\" + 0.100*\"littl\" + 0.092*\"great\"\n",
      "\n",
      "Score:0.44865756945822954\t \n",
      "Topic:-0.801*\"seat\" + 0.328*\"babi\" + -0.196*\"stroller\" + 0.150*\"bottl\" + 0.115*\"diaper\" + -0.084*\"strap\" + 0.083*\"love\" + 0.083*\"pump\" + 0.082*\"work\" + 0.080*\"monitor\"\n",
      "\n",
      "Score:0.35557882324727114\t \n",
      "Topic:-0.619*\"gate\" + 0.440*\"monitor\" + -0.209*\"open\" + -0.172*\"easi\" + 0.169*\"camera\" + 0.155*\"bottl\" + 0.135*\"diaper\" + 0.135*\"love\" + 0.132*\"night\" + -0.115*\"instal\"\n",
      "\n",
      "Score:0.3544947932126671\t \n",
      "Topic:-0.526*\"pillow\" + 0.374*\"great\" + -0.349*\"sleep\" + 0.265*\"chair\" + -0.210*\"like\" + 0.199*\"easi\" + 0.190*\"monitor\" + -0.141*\"night\" + 0.121*\"clean\" + 0.111*\"product\"\n",
      "\n",
      "Score:0.24586650099389495\t \n",
      "Topic:0.688*\"stroller\" + -0.514*\"babi\" + -0.303*\"seat\" + 0.163*\"diaper\" + 0.104*\"fold\" + 0.086*\"wheel\" + 0.075*\"great\" + -0.066*\"chair\" + 0.064*\"handl\" + -0.061*\"carrier\"\n",
      "\n",
      "Score:0.15492116802276412\t \n",
      "Topic:-0.529*\"babi\" + -0.450*\"stroller\" + 0.421*\"bottl\" + 0.328*\"pump\" + 0.154*\"work\" + 0.116*\"nippl\" + 0.113*\"seat\" + 0.111*\"milk\" + 0.099*\"time\" + 0.093*\"clean\"\n",
      "\n",
      "Score:-0.5362449374989888\t \n",
      "Topic:-0.742*\"pump\" + 0.352*\"bottl\" + 0.230*\"like\" + -0.215*\"love\" + -0.154*\"diaper\" + -0.135*\"medela\" + -0.118*\"babi\" + 0.111*\"gate\" + -0.100*\"milk\" + 0.090*\"nippl\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lsi_model[bow_corpus[4310]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lsi_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f2a3cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6999167203903198\t Topic: 0.040*\"diaper\" + 0.018*\"wash\" + 0.016*\"cover\" + 0.016*\"like\" + 0.015*\"cloth\"\n",
      "Score: 0.03335949033498764\t Topic: 0.043*\"love\" + 0.033*\"babi\" + 0.029*\"play\" + 0.024*\"month\" + 0.018*\"like\"\n",
      "Score: 0.033356599509716034\t Topic: 0.038*\"love\" + 0.027*\"great\" + 0.026*\"look\" + 0.022*\"color\" + 0.022*\"babi\"\n",
      "Score: 0.0333452969789505\t Topic: 0.024*\"babi\" + 0.020*\"love\" + 0.018*\"like\" + 0.017*\"food\" + 0.015*\"clean\"\n",
      "Score: 0.03334128111600876\t Topic: 0.061*\"seat\" + 0.036*\"stroller\" + 0.016*\"easi\" + 0.014*\"strap\" + 0.013*\"love\"\n",
      "Score: 0.033338505774736404\t Topic: 0.019*\"potti\" + 0.018*\"seat\" + 0.012*\"year\" + 0.012*\"littl\" + 0.012*\"like\"\n",
      "Score: 0.03333756700158119\t Topic: 0.031*\"babi\" + 0.027*\"sleep\" + 0.025*\"pump\" + 0.017*\"time\" + 0.016*\"work\"\n",
      "Score: 0.03333590179681778\t Topic: 0.078*\"bottl\" + 0.022*\"nippl\" + 0.021*\"mattress\" + 0.014*\"milk\" + 0.012*\"like\"\n",
      "Score: 0.0333343967795372\t Topic: 0.028*\"monitor\" + 0.017*\"work\" + 0.016*\"babi\" + 0.014*\"product\" + 0.012*\"camera\"\n",
      "Score: 0.03333420678973198\t Topic: 0.018*\"chair\" + 0.014*\"open\" + 0.013*\"easi\" + 0.011*\"work\" + 0.010*\"gate\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'I love this diaper so much '\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3011d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.019397391867389195\t Topic: 0.028*\"monitor\" + 0.017*\"work\" + 0.016*\"babi\" + 0.014*\"product\" + 0.012*\"camera\"\n",
      "Score: 0.008032488050040585\t Topic: 0.038*\"love\" + 0.027*\"great\" + 0.026*\"look\" + 0.022*\"color\" + 0.022*\"babi\"\n",
      "Score: 0.0075336813871248525\t Topic: 0.043*\"love\" + 0.033*\"babi\" + 0.029*\"play\" + 0.024*\"month\" + 0.018*\"like\"\n",
      "Score: 0.005687626033973621\t Topic: 0.018*\"chair\" + 0.014*\"open\" + 0.013*\"easi\" + 0.011*\"work\" + 0.010*\"gate\"\n",
      "Score: 0.0044730668367517455\t Topic: 0.078*\"bottl\" + 0.022*\"nippl\" + 0.021*\"mattress\" + 0.014*\"milk\" + 0.012*\"like\"\n",
      "Score: 0.0010650655735868694\t Topic: 0.061*\"seat\" + 0.036*\"stroller\" + 0.016*\"easi\" + 0.014*\"strap\" + 0.013*\"love\"\n",
      "Score: 0.0002588907024039404\t Topic: 0.019*\"potti\" + 0.018*\"seat\" + 0.012*\"year\" + 0.012*\"littl\" + 0.012*\"like\"\n",
      "Score: -0.00017510167995988017\t Topic: 0.031*\"babi\" + 0.027*\"sleep\" + 0.025*\"pump\" + 0.017*\"time\" + 0.016*\"work\"\n",
      "Score: -0.000835770033620649\t Topic: 0.024*\"babi\" + 0.020*\"love\" + 0.018*\"like\" + 0.017*\"food\" + 0.015*\"clean\"\n",
      "Score: -0.010729565912471657\t Topic: 0.040*\"diaper\" + 0.018*\"wash\" + 0.016*\"cover\" + 0.016*\"like\" + 0.015*\"cloth\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lsi_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61b3c038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>diaper, wash, cover, cloth, like, babi, size, ...</td>\n",
       "      <td>[flannel, wipe, opinion, worth, keep, order, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>diaper, wash, cover, cloth, like, babi, size, ...</td>\n",
       "      <td>[come, earli, disappoint, love, planet, wise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.7656</td>\n",
       "      <td>love, look, great, color, babi, qualiti, cute,...</td>\n",
       "      <td>[soft, comfort, warmer, look, size, perfectli,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>love, look, great, color, babi, qualiti, cute,...</td>\n",
       "      <td>[product, worth, purchas, like, posit, ingeni,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4520</td>\n",
       "      <td>love, babi, food, like, clean, easi, great, li...</td>\n",
       "      <td>[kid, cri, stop, tri, ween, pacifi, thumbuddi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>love, babi, food, like, clean, easi, great, li...</td>\n",
       "      <td>[binki, fairi, come, hous, special, gift, book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4780</td>\n",
       "      <td>love, look, great, color, babi, qualiti, cute,...</td>\n",
       "      <td>[love, book, bind, tightli, abl, alot, photo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8327</td>\n",
       "      <td>babi, sleep, pump, time, comfort, pillow, work...</td>\n",
       "      <td>[perfect, parent, abl, track, babi, feed, slee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>babi, sleep, pump, time, comfort, pillow, work...</td>\n",
       "      <td>[friend, pin, product, pinterest, decid, whirl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>monitor, work, babi, product, camera, time, re...</td>\n",
       "      <td>[easi, nanni, record, event, happen, babi, hom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             6.0              0.8077   \n",
       "1            1             6.0              0.9437   \n",
       "2            2             7.0              0.7656   \n",
       "3            3             7.0              0.3779   \n",
       "4            4             5.0              0.4520   \n",
       "5            5             5.0              0.3612   \n",
       "6            6             7.0              0.4780   \n",
       "7            7             4.0              0.8327   \n",
       "8            8             4.0              0.4961   \n",
       "9            9             0.0              0.6393   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  diaper, wash, cover, cloth, like, babi, size, ...   \n",
       "1  diaper, wash, cover, cloth, like, babi, size, ...   \n",
       "2  love, look, great, color, babi, qualiti, cute,...   \n",
       "3  love, look, great, color, babi, qualiti, cute,...   \n",
       "4  love, babi, food, like, clean, easi, great, li...   \n",
       "5  love, babi, food, like, clean, easi, great, li...   \n",
       "6  love, look, great, color, babi, qualiti, cute,...   \n",
       "7  babi, sleep, pump, time, comfort, pillow, work...   \n",
       "8  babi, sleep, pump, time, comfort, pillow, work...   \n",
       "9  monitor, work, babi, product, camera, time, re...   \n",
       "\n",
       "                                                Text  \n",
       "0  [flannel, wipe, opinion, worth, keep, order, s...  \n",
       "1  [come, earli, disappoint, love, planet, wise, ...  \n",
       "2  [soft, comfort, warmer, look, size, perfectli,...  \n",
       "3  [product, worth, purchas, like, posit, ingeni,...  \n",
       "4  [kid, cri, stop, tri, ween, pacifi, thumbuddi,...  \n",
       "5  [binki, fairi, come, hous, special, gift, book...  \n",
       "6  [love, book, bind, tightli, abl, alot, photo, ...  \n",
       "7  [perfect, parent, abl, track, babi, feed, slee...  \n",
       "8  [friend, pin, product, pinterest, decid, whirl...  \n",
       "9  [easi, nanni, record, event, happen, babi, hom...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topic_sentences(lda_model=None, corpus=bow_corpus, texts=processed_docs):\n",
    "    sent_topics_df=pd.DataFrame()\n",
    "    \n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        row=row_list[0] if lda_model.per_word_topics else row_list\n",
    "        row=sorted(row,key=lambda x:(x[1]),reverse=True)\n",
    "        \n",
    "        for j, (topic_num,prop_topic) in enumerate(row):\n",
    "            if j==0:\n",
    "                wp=lda_model.show_topic(topic_num)\n",
    "                topic_keywords=', '.join([word for word, prop in wp])\n",
    "                sent_topics_df=sent_topics_df.append(pd.Series([int(topic_num),round(prop_topic,4),topic_keywords]),ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns=['Dominant_topic','Perc_contribution','Topic_keywords']\n",
    "    contents=pd.Series(texts)\n",
    "    sent_topics_df=pd.concat([sent_topics_df,contents],axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topic_sentences(lda_model=lda_model, corpus=bow_corpus, texts=processed_docs)\n",
    "\n",
    "# Format.  \n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58594bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41a0aeb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Dominant_Topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-71245326ee13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msent_topics_sorteddf_mallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msent_topics_outdf_grpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_topic_sents_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dominant_Topic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_topics_outdf_grpd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6715\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6717\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   6718\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6719\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    561\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Dominant_Topic'"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "print(sent_topics_sorteddf_mallet.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopwords,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a66fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2eb1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f6f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5e543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ae65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060444d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6bedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46c9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8116273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9982a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b59af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fdab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
