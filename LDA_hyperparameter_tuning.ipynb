{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fcfb810b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>1</td>\n",
       "      <td>Diapering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>2</td>\n",
       "      <td>Nursery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>4</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>654</td>\n",
       "      <td>U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...</td>\n",
       "      <td>The sweaters are very nice and fit quite well....</td>\n",
       "      <td>654</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>658</td>\n",
       "      <td>U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...</td>\n",
       "      <td>My 13 month old is short and round lol. So I’m...</td>\n",
       "      <td>658</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>660</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>This is sp cute and hilarious 100 recomend goo...</td>\n",
       "      <td>660</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>661</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>Mr. B is only 4 weeks old and looks so handsom...</td>\n",
       "      <td>661</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>663</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>I wanted to dress up my guinea pig but the swe...</td>\n",
       "      <td>663</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               name  \\\n",
       "0             0                           Planetwise Flannel Wipes   \n",
       "1             1                              Planetwise Wipe Pouch   \n",
       "2             2                Annas Dream Full Quilt with 2 Shams   \n",
       "3             3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4             4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "..          ...                                                ...   \n",
       "408         654  U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...   \n",
       "409         658  U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...   \n",
       "410         660  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "411         661  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "412         663  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "\n",
       "                                                review  index  \\\n",
       "0    These flannel wipes are OK, but in my opinion ...      0   \n",
       "1    it came early and was not disappointed. i love...      1   \n",
       "2    Very soft and comfortable and warmer than it l...      2   \n",
       "3    This is a product well worth the purchase.  I ...      3   \n",
       "4    All of my kids have cried non-stop when I trie...      4   \n",
       "..                                                 ...    ...   \n",
       "408  The sweaters are very nice and fit quite well....    654   \n",
       "409  My 13 month old is short and round lol. So I’m...    658   \n",
       "410  This is sp cute and hilarious 100 recomend goo...    660   \n",
       "411  Mr. B is only 4 weeks old and looks so handsom...    661   \n",
       "412  I wanted to dress up my guinea pig but the swe...    663   \n",
       "\n",
       "                Category  \n",
       "0              Baby_Care  \n",
       "1              Diapering  \n",
       "2                Nursery  \n",
       "3              Baby_Care  \n",
       "4              Baby_Care  \n",
       "..                   ...  \n",
       "408  Apparel_accessories  \n",
       "409  Apparel_accessories  \n",
       "410  Apparel_accessories  \n",
       "411  Apparel_accessories  \n",
       "412  Apparel_accessories  \n",
       "\n",
       "[413 rows x 5 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "doc=pd.read_csv(\"used4.5.csv\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "20caa0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-318acfee2307>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index']=data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text=doc[[\"review\",\"Category\"]]\n",
    "data_text['index']=data_text.index\n",
    "\n",
    "documents=data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bbe87796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Category</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>Diapering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>The sweaters are very nice and fit quite well....</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>My 13 month old is short and round lol. So I’m...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>This is sp cute and hilarious 100 recomend goo...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Mr. B is only 4 weeks old and looks so handsom...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>I wanted to dress up my guinea pig but the swe...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review             Category  \\\n",
       "0    These flannel wipes are OK, but in my opinion ...            Baby_Care   \n",
       "1    it came early and was not disappointed. i love...            Diapering   \n",
       "2    Very soft and comfortable and warmer than it l...              Nursery   \n",
       "3    This is a product well worth the purchase.  I ...            Baby_Care   \n",
       "4    All of my kids have cried non-stop when I trie...            Baby_Care   \n",
       "..                                                 ...                  ...   \n",
       "408  The sweaters are very nice and fit quite well....  Apparel_accessories   \n",
       "409  My 13 month old is short and round lol. So I’m...  Apparel_accessories   \n",
       "410  This is sp cute and hilarious 100 recomend goo...  Apparel_accessories   \n",
       "411  Mr. B is only 4 weeks old and looks so handsom...  Apparel_accessories   \n",
       "412  I wanted to dress up my guinea pig but the swe...  Apparel_accessories   \n",
       "\n",
       "     index  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "..     ...  \n",
       "408    408  \n",
       "409    409  \n",
       "410    410  \n",
       "411    411  \n",
       "412    412  \n",
       "\n",
       "[413 rows x 3 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2a8bf1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk.stem as stemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9add39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "      return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >3:\n",
    "              result.append(lemmatize_stemming(token))\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e6730d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document\n",
      "['All', 'of', 'my', 'kids', 'have', 'cried', 'non-stop', 'when', 'I', 'tried', 'to', 'ween', 'them', 'off', 'their', 'pacifier,', 'until', 'I', 'found', 'Thumbuddy', 'To', \"Love's\", 'Binky', 'Fairy', 'Puppet.', '', 'It', 'is', 'an', 'easy', 'way', 'to', 'work', 'with', 'your', 'kids', 'to', 'allow', 'them', 'to', 'understand', 'where', 'their', 'pacifier', 'is', 'going', 'and', 'help', 'them', 'part', 'from', 'it.This', 'is', 'a', 'must', 'buy', 'book,', 'and', 'a', 'great', 'gift', 'for', 'expecting', 'parents!!', '', 'You', 'will', 'save', 'them', 'soo', 'many', 'headaches.Thanks', 'for', 'this', 'book!', '', 'You', 'all', 'rock!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['kid', 'cri', 'stop', 'tri', 'ween', 'pacifi', 'thumbuddi', 'love', 'binki', 'fairi', 'puppet', 'easi', 'work', 'kid', 'allow', 'understand', 'pacifi', 'go', 'help', 'book', 'great', 'gift', 'expect', 'parent', 'save', 'headach', 'thank', 'book', 'rock']\n"
     ]
    }
   ],
   "source": [
    "doc_sample=documents[documents['index']==4].values[0][0]\n",
    "print('original document')\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "      words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "819010ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-192-00cd468fc166>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
     ]
    }
   ],
   "source": [
    "documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "dbdb4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [flannel, wipe, opinion, worth, keep, order, s...\n",
       "1      [come, earli, disappoint, love, planet, wise, ...\n",
       "2      [soft, comfort, warmer, look, size, perfectli,...\n",
       "3      [product, worth, purchas, like, posit, ingeni,...\n",
       "4      [kid, cri, stop, tri, ween, pacifi, thumbuddi,...\n",
       "                             ...                        \n",
       "408    [sweater, nice, ferret, mind, fabric, great, p...\n",
       "409    [month, short, round, find, difficult, pant, p...\n",
       "410    [cute, hilari, recomend, good, ador, halloween...\n",
       "411    [week, look, handsom, littl, sweater, hard, sw...\n",
       "412    [want, dress, guinea, sweater, small, especi, ...\n",
       "Name: review, Length: 413, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['review'].map(preprocess)\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e1e89cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blue\n",
      "1 boyfor\n",
      "2 cloth\n",
      "3 countwhich\n",
      "4 face\n",
      "5 flannel\n",
      "6 hand\n",
      "7 handl\n",
      "8 higher\n",
      "9 issu\n",
      "10 keep\n",
      "11 larger\n",
      "12 longer\n",
      "13 month\n",
      "14 need\n",
      "15 nicer\n",
      "16 ocean\n",
      "17 opinion\n",
      "18 order\n",
      "19 pack\n",
      "20 qualiti\n"
     ]
    }
   ],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)\n",
    "count=0\n",
    "\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "18dcfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f9c2a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 1),\n",
       " (24, 2),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 2),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "18dbca12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 13 (\"love\") appears 1 time.\n",
      "Word 24 (\"book\") appears 2 time.\n",
      "Word 25 (\"easi\") appears 1 time.\n",
      "Word 26 (\"expect\") appears 1 time.\n",
      "Word 27 (\"gift\") appears 1 time.\n",
      "Word 28 (\"go\") appears 1 time.\n",
      "Word 29 (\"great\") appears 1 time.\n",
      "Word 30 (\"help\") appears 1 time.\n",
      "Word 31 (\"kid\") appears 2 time.\n",
      "Word 32 (\"parent\") appears 1 time.\n",
      "Word 33 (\"tri\") appears 1 time.\n",
      "Word 34 (\"work\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4=bow_corpus[4]\n",
    "for i in range(len(bow_doc_4)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4[i][0], \n",
    "                                               dictionary[bow_doc_4[i][0]], \n",
    "bow_doc_4[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "71cd26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim import corpora, models\n",
    "# tfidf=models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf=tfidf[bow_corpus]\n",
    "\n",
    "# for i in range(len(corpus_tfidf)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(corpus_tfidf[i][0], \n",
    "#                                                dictionary[corpus_tfidf[i][0]], \n",
    "# corpus_tfidf[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "993f4cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.3984230501974015), (13, 0.3715174863580215), (18, 0.2533365765774925), (19, 0.33533829628088985), (20, 0.1900094157584384), (21, 0.2354594241008233), (22, 0.5579205528408278), (23, 0.3518274785460531)]\n",
      "[(0, 0.37378287425890977),\n",
      " (1, 0.2910888830907095),\n",
      " (2, 0.32540999039054613),\n",
      " (3, 0.1454802833186967),\n",
      " (4, 0.2846385440521788),\n",
      " (5, 0.36733253522037906),\n",
      " (6, 0.36121313179202574),\n",
      " (7, 0.31688477799387366),\n",
      " (8, 0.28153961109325704),\n",
      " (9, 0.3498424166134522)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf=tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "print(corpus_tfidf[3])\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "191269ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.025*\"love\" + 0.024*\"book\" + 0.023*\"month\" + 0.022*\"babi\" + 0.022*\"high\" + 0.022*\"learn\" + 0.022*\"great\" + 0.021*\"buy\" + 0.019*\"item\" + 0.017*\"time\"\n",
      "Topic: 1 Word: 0.042*\"sling\" + 0.032*\"gift\" + 0.027*\"babi\" + 0.027*\"monitor\" + 0.025*\"camera\" + 0.021*\"great\" + 0.019*\"play\" + 0.016*\"like\" + 0.016*\"give\" + 0.016*\"month\"\n",
      "Topic: 2 Word: 0.039*\"book\" + 0.029*\"year\" + 0.025*\"like\" + 0.021*\"look\" + 0.021*\"babi\" + 0.021*\"diaper\" + 0.020*\"page\" + 0.020*\"happi\" + 0.019*\"love\" + 0.017*\"track\"\n",
      "Topic: 3 Word: 0.039*\"play\" + 0.025*\"great\" + 0.022*\"easi\" + 0.021*\"favorit\" + 0.020*\"look\" + 0.019*\"item\" + 0.019*\"go\" + 0.018*\"like\" + 0.018*\"babi\" + 0.018*\"month\"\n",
      "Topic: 4 Word: 0.031*\"love\" + 0.023*\"easi\" + 0.022*\"babi\" + 0.022*\"need\" + 0.020*\"disappoint\" + 0.019*\"room\" + 0.019*\"nice\" + 0.019*\"good\" + 0.019*\"year\" + 0.018*\"littl\"\n",
      "Topic: 5 Word: 0.034*\"book\" + 0.030*\"price\" + 0.028*\"good\" + 0.028*\"qualiti\" + 0.027*\"famili\" + 0.027*\"cute\" + 0.026*\"teeth\" + 0.023*\"interact\" + 0.021*\"vibrat\" + 0.020*\"play\"\n",
      "Topic: 6 Word: 0.027*\"perfect\" + 0.027*\"diaper\" + 0.025*\"book\" + 0.023*\"think\" + 0.022*\"daughter\" + 0.021*\"babi\" + 0.021*\"year\" + 0.020*\"start\" + 0.019*\"receiv\" + 0.019*\"love\"\n",
      "Topic: 7 Word: 0.049*\"teether\" + 0.033*\"stick\" + 0.031*\"free\" + 0.026*\"vibrat\" + 0.025*\"buy\" + 0.024*\"help\" + 0.024*\"hard\" + 0.023*\"teeth\" + 0.018*\"like\" + 0.018*\"daughter\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf=gensim.models.LdaMulticore(corpus_tfidf,num_topics=8,id2word=dictionary,passes=2,workers=4)\n",
    "# lda_model_tfidf = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "#                                       max_iter=10,               # Max learning iterations\n",
    "#                                       learning_method='online',   \n",
    "#                                       random_state=100,          # Random state\n",
    "#                                       batch_size=128,            # n docs in each learning iter\n",
    "#                                       evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "#                                       n_jobs = -1,               # Use all available CPUs\n",
    "                                     \n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e8e38f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"love\" + 0.017*\"book\" + 0.017*\"buy\" + 0.015*\"diaper\" + 0.014*\"month\" + 0.014*\"babi\" + 0.014*\"play\" + 0.014*\"look\" + 0.014*\"like\" + 0.013*\"gift\"\n",
      "Topic: 1 Word: 0.019*\"book\" + 0.019*\"babi\" + 0.015*\"like\" + 0.015*\"play\" + 0.014*\"love\" + 0.014*\"great\" + 0.014*\"look\" + 0.014*\"littl\" + 0.012*\"month\" + 0.012*\"diaper\"\n",
      "Topic: 2 Word: 0.023*\"babi\" + 0.017*\"like\" + 0.014*\"great\" + 0.014*\"love\" + 0.013*\"need\" + 0.013*\"month\" + 0.012*\"year\" + 0.012*\"perfect\" + 0.012*\"look\" + 0.012*\"think\"\n",
      "Topic: 3 Word: 0.021*\"babi\" + 0.019*\"love\" + 0.013*\"perfect\" + 0.013*\"play\" + 0.013*\"month\" + 0.013*\"cute\" + 0.013*\"book\" + 0.012*\"like\" + 0.012*\"great\" + 0.011*\"time\"\n",
      "Topic: 4 Word: 0.017*\"book\" + 0.016*\"buy\" + 0.016*\"babi\" + 0.015*\"great\" + 0.014*\"love\" + 0.013*\"month\" + 0.013*\"year\" + 0.013*\"littl\" + 0.012*\"look\" + 0.012*\"time\"\n",
      "Topic: 5 Word: 0.017*\"babi\" + 0.016*\"daughter\" + 0.016*\"book\" + 0.015*\"great\" + 0.015*\"like\" + 0.015*\"monitor\" + 0.015*\"littl\" + 0.015*\"play\" + 0.014*\"love\" + 0.014*\"month\"\n",
      "Topic: 6 Word: 0.021*\"babi\" + 0.019*\"diaper\" + 0.018*\"month\" + 0.018*\"think\" + 0.017*\"love\" + 0.016*\"year\" + 0.016*\"book\" + 0.015*\"littl\" + 0.014*\"like\" + 0.013*\"great\"\n",
      "Topic: 7 Word: 0.019*\"love\" + 0.017*\"book\" + 0.014*\"babi\" + 0.014*\"year\" + 0.013*\"play\" + 0.013*\"great\" + 0.013*\"littl\" + 0.013*\"month\" + 0.013*\"like\" + 0.012*\"time\"\n"
     ]
    }
   ],
   "source": [
    "alpha_list = ['symmetric',0.3,0.5,0.7]\n",
    "beta_list = ['auto',0.3,0.5,0.7]\n",
    "lda_model_tfidf=gensim.models.LdaMulticore(\n",
    "    corpus_tfidf,\n",
    "    num_topics=8,\n",
    "    id2word=dictionary,\n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    alpha=alpha_list[2],\n",
    "    eta=beta_list[2],\n",
    "\n",
    "    passes=2,\n",
    "    workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9402fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.6539245247840881\t \n",
      "Topic:0.019*\"book\" + 0.019*\"babi\" + 0.015*\"like\" + 0.015*\"play\" + 0.014*\"love\" + 0.014*\"great\" + 0.014*\"look\" + 0.014*\"littl\" + 0.012*\"month\" + 0.012*\"diaper\"\n",
      "\n",
      "Score:0.054598644375801086\t \n",
      "Topic:0.021*\"babi\" + 0.019*\"diaper\" + 0.018*\"month\" + 0.018*\"think\" + 0.017*\"love\" + 0.016*\"year\" + 0.016*\"book\" + 0.015*\"littl\" + 0.014*\"like\" + 0.013*\"great\"\n",
      "\n",
      "Score:0.05102014169096947\t \n",
      "Topic:0.019*\"love\" + 0.017*\"book\" + 0.014*\"babi\" + 0.014*\"year\" + 0.013*\"play\" + 0.013*\"great\" + 0.013*\"littl\" + 0.013*\"month\" + 0.013*\"like\" + 0.012*\"time\"\n",
      "\n",
      "Score:0.050463780760765076\t \n",
      "Topic:0.017*\"book\" + 0.016*\"buy\" + 0.016*\"babi\" + 0.015*\"great\" + 0.014*\"love\" + 0.013*\"month\" + 0.013*\"year\" + 0.013*\"littl\" + 0.012*\"look\" + 0.012*\"time\"\n",
      "\n",
      "Score:0.050225820392370224\t \n",
      "Topic:0.017*\"babi\" + 0.016*\"daughter\" + 0.016*\"book\" + 0.015*\"great\" + 0.015*\"like\" + 0.015*\"monitor\" + 0.015*\"littl\" + 0.015*\"play\" + 0.014*\"love\" + 0.014*\"month\"\n",
      "\n",
      "Score:0.048948023468256\t \n",
      "Topic:0.020*\"love\" + 0.017*\"book\" + 0.017*\"buy\" + 0.015*\"diaper\" + 0.014*\"month\" + 0.014*\"babi\" + 0.014*\"play\" + 0.014*\"look\" + 0.014*\"like\" + 0.013*\"gift\"\n",
      "\n",
      "Score:0.04829651117324829\t \n",
      "Topic:0.021*\"babi\" + 0.019*\"love\" + 0.013*\"perfect\" + 0.013*\"play\" + 0.013*\"month\" + 0.013*\"cute\" + 0.013*\"book\" + 0.012*\"like\" + 0.012*\"great\" + 0.011*\"time\"\n",
      "\n",
      "Score:0.042522527277469635\t \n",
      "Topic:0.023*\"babi\" + 0.017*\"like\" + 0.014*\"great\" + 0.014*\"love\" + 0.013*\"need\" + 0.013*\"month\" + 0.012*\"year\" + 0.012*\"perfect\" + 0.012*\"look\" + 0.012*\"think\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b8bc0069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.13788829743862152\t \n",
      "Topic:0.017*\"book\" + 0.016*\"buy\" + 0.016*\"babi\" + 0.015*\"great\" + 0.014*\"love\"\n",
      "\n",
      "\n",
      "Score:0.13361935317516327\t \n",
      "Topic:0.019*\"book\" + 0.019*\"babi\" + 0.015*\"like\" + 0.015*\"play\" + 0.014*\"love\"\n",
      "\n",
      "\n",
      "Score:0.13211484253406525\t \n",
      "Topic:0.023*\"babi\" + 0.017*\"like\" + 0.014*\"great\" + 0.014*\"love\" + 0.013*\"need\"\n",
      "\n",
      "\n",
      "Score:0.12450427561998367\t \n",
      "Topic:0.020*\"love\" + 0.017*\"book\" + 0.017*\"buy\" + 0.015*\"diaper\" + 0.014*\"month\"\n",
      "\n",
      "\n",
      "Score:0.12171528488397598\t \n",
      "Topic:0.021*\"babi\" + 0.019*\"love\" + 0.013*\"perfect\" + 0.013*\"play\" + 0.013*\"month\"\n",
      "\n",
      "\n",
      "Score:0.11815974116325378\t \n",
      "Topic:0.021*\"babi\" + 0.019*\"diaper\" + 0.018*\"month\" + 0.018*\"think\" + 0.017*\"love\"\n",
      "\n",
      "\n",
      "Score:0.11718123406171799\t \n",
      "Topic:0.019*\"love\" + 0.017*\"book\" + 0.014*\"babi\" + 0.014*\"year\" + 0.013*\"play\"\n",
      "\n",
      "\n",
      "Score:0.11481700092554092\t \n",
      "Topic:0.017*\"babi\" + 0.016*\"daughter\" + 0.016*\"book\" + 0.015*\"great\" + 0.015*\"like\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'This wallpaper has always been one of my favorite  '\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    \n",
    "        print(\"\\nScore:{}\\t \\nTopic:{}\\n\".format(score,lda_model_tfidf.print_topic(index,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6b5435ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>babi, like, great, love, need, month, year, pe...</td>\n",
       "      <td>[flannel, wipe, opinion, worth, keep, order, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1716</td>\n",
       "      <td>babi, diaper, month, think, love, year, book, ...</td>\n",
       "      <td>[come, earli, disappoint, love, planet, wise, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3321</td>\n",
       "      <td>book, babi, like, play, love, great, look, lit...</td>\n",
       "      <td>[soft, comfort, warmer, look, size, perfectli,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>babi, diaper, month, think, love, year, book, ...</td>\n",
       "      <td>[product, worth, purchas, like, posit, ingeni,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             2.0              0.2944   \n",
       "1            1             6.0              0.1716   \n",
       "2            2             1.0              0.3321   \n",
       "3            3             6.0              0.4027   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  babi, like, great, love, need, month, year, pe...   \n",
       "1  babi, diaper, month, think, love, year, book, ...   \n",
       "2  book, babi, like, play, love, great, look, lit...   \n",
       "3  babi, diaper, month, think, love, year, book, ...   \n",
       "\n",
       "                                                Text  index  \n",
       "0  [flannel, wipe, opinion, worth, keep, order, s...      0  \n",
       "1  [come, earli, disappoint, love, planet, wise, ...      1  \n",
       "2  [soft, comfort, warmer, look, size, perfectli,...      2  \n",
       "3  [product, worth, purchas, like, posit, ingeni,...      3  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topic_sentences(lda_model=lda_model_tfidf, corpus=bow_corpus, texts=processed_docs):\n",
    "    sent_topics_df=pd.DataFrame()\n",
    "    \n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        row=row_list[0] if lda_model.per_word_topics else row_list\n",
    "        row=sorted(row,key=lambda x:(x[1]),reverse=True)\n",
    "        \n",
    "        for j, (topic_num,prop_topic) in enumerate(row):\n",
    "            if j==0:\n",
    "                wp=lda_model.show_topic(topic_num)\n",
    "                topic_keywords=', '.join([word for word, prop in wp])\n",
    "                sent_topics_df=sent_topics_df.append(pd.Series([int(topic_num),round(prop_topic,4),topic_keywords]),ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns=['Dominant_topic','Perc_contribution','Topic_keywords']\n",
    "    contents=pd.Series(texts)\n",
    "    sent_topics_df=pd.concat([sent_topics_df,contents],axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topic_sentences(lda_model=lda_model_tfidf, corpus=bow_corpus, texts=processed_docs)\n",
    "\n",
    "# Format.  \n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic['index']=df_dominant_topic.index\n",
    "df_dominant_topic.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596d632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf70f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8711031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1379a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1dd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
