{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "742c8073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>1</td>\n",
       "      <td>Diapering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>2</td>\n",
       "      <td>Nursery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>4</td>\n",
       "      <td>Baby_Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>654</td>\n",
       "      <td>U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...</td>\n",
       "      <td>The sweaters are very nice and fit quite well....</td>\n",
       "      <td>654</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>658</td>\n",
       "      <td>U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...</td>\n",
       "      <td>My 13 month old is short and round lol. So I’m...</td>\n",
       "      <td>658</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>660</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>This is sp cute and hilarious 100 recomend goo...</td>\n",
       "      <td>660</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>661</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>Mr. B is only 4 weeks old and looks so handsom...</td>\n",
       "      <td>661</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>663</td>\n",
       "      <td>HAICHEN TEC Ferret Clothes Turtleneck Sweater ...</td>\n",
       "      <td>I wanted to dress up my guinea pig but the swe...</td>\n",
       "      <td>663</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               name  \\\n",
       "0             0                           Planetwise Flannel Wipes   \n",
       "1             1                              Planetwise Wipe Pouch   \n",
       "2             2                Annas Dream Full Quilt with 2 Shams   \n",
       "3             3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4             4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "..          ...                                                ...   \n",
       "408         654  U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...   \n",
       "409         658  U·nikaka Unisex Baby 0-48 Months 5-Pack Pants ...   \n",
       "410         660  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "411         661  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "412         663  HAICHEN TEC Ferret Clothes Turtleneck Sweater ...   \n",
       "\n",
       "                                                review  index  \\\n",
       "0    These flannel wipes are OK, but in my opinion ...      0   \n",
       "1    it came early and was not disappointed. i love...      1   \n",
       "2    Very soft and comfortable and warmer than it l...      2   \n",
       "3    This is a product well worth the purchase.  I ...      3   \n",
       "4    All of my kids have cried non-stop when I trie...      4   \n",
       "..                                                 ...    ...   \n",
       "408  The sweaters are very nice and fit quite well....    654   \n",
       "409  My 13 month old is short and round lol. So I’m...    658   \n",
       "410  This is sp cute and hilarious 100 recomend goo...    660   \n",
       "411  Mr. B is only 4 weeks old and looks so handsom...    661   \n",
       "412  I wanted to dress up my guinea pig but the swe...    663   \n",
       "\n",
       "                Category  \n",
       "0              Baby_Care  \n",
       "1              Diapering  \n",
       "2                Nursery  \n",
       "3              Baby_Care  \n",
       "4              Baby_Care  \n",
       "..                   ...  \n",
       "408  Apparel_accessories  \n",
       "409  Apparel_accessories  \n",
       "410  Apparel_accessories  \n",
       "411  Apparel_accessories  \n",
       "412  Apparel_accessories  \n",
       "\n",
       "[413 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "doc=pd.read_csv(\"used4.5.csv\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cb191f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-318acfee2307>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index']=data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text=doc[[\"review\",\"Category\"]]\n",
    "data_text['index']=data_text.index\n",
    "\n",
    "documents=data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebba7f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Category</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>Diapering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>The sweaters are very nice and fit quite well....</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>My 13 month old is short and round lol. So I’m...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>This is sp cute and hilarious 100 recomend goo...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Mr. B is only 4 weeks old and looks so handsom...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>I wanted to dress up my guinea pig but the swe...</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review             Category  \\\n",
       "0    These flannel wipes are OK, but in my opinion ...            Baby_Care   \n",
       "1    it came early and was not disappointed. i love...            Diapering   \n",
       "2    Very soft and comfortable and warmer than it l...              Nursery   \n",
       "3    This is a product well worth the purchase.  I ...            Baby_Care   \n",
       "4    All of my kids have cried non-stop when I trie...            Baby_Care   \n",
       "..                                                 ...                  ...   \n",
       "408  The sweaters are very nice and fit quite well....  Apparel_accessories   \n",
       "409  My 13 month old is short and round lol. So I’m...  Apparel_accessories   \n",
       "410  This is sp cute and hilarious 100 recomend goo...  Apparel_accessories   \n",
       "411  Mr. B is only 4 weeks old and looks so handsom...  Apparel_accessories   \n",
       "412  I wanted to dress up my guinea pig but the swe...  Apparel_accessories   \n",
       "\n",
       "     index  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "..     ...  \n",
       "408    408  \n",
       "409    409  \n",
       "410    410  \n",
       "411    411  \n",
       "412    412  \n",
       "\n",
       "[413 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfef0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk.stem as stemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "647fd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "      return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >3:\n",
    "              result.append(lemmatize_stemming(token))\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14889247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document\n",
      "['All', 'of', 'my', 'kids', 'have', 'cried', 'non-stop', 'when', 'I', 'tried', 'to', 'ween', 'them', 'off', 'their', 'pacifier,', 'until', 'I', 'found', 'Thumbuddy', 'To', \"Love's\", 'Binky', 'Fairy', 'Puppet.', '', 'It', 'is', 'an', 'easy', 'way', 'to', 'work', 'with', 'your', 'kids', 'to', 'allow', 'them', 'to', 'understand', 'where', 'their', 'pacifier', 'is', 'going', 'and', 'help', 'them', 'part', 'from', 'it.This', 'is', 'a', 'must', 'buy', 'book,', 'and', 'a', 'great', 'gift', 'for', 'expecting', 'parents!!', '', 'You', 'will', 'save', 'them', 'soo', 'many', 'headaches.Thanks', 'for', 'this', 'book!', '', 'You', 'all', 'rock!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['kid', 'cri', 'stop', 'tri', 'ween', 'pacifi', 'thumbuddi', 'love', 'binki', 'fairi', 'puppet', 'easi', 'work', 'kid', 'allow', 'understand', 'pacifi', 'go', 'help', 'book', 'great', 'gift', 'expect', 'parent', 'save', 'headach', 'thank', 'book', 'rock']\n"
     ]
    }
   ],
   "source": [
    "doc_sample=documents[documents['index']==4].values[0][0]\n",
    "print('original document')\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "      words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98067b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-00cd468fc166>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
     ]
    }
   ],
   "source": [
    "documents.dropna(subset = [\"review\"], inplace=True) # drop those rows which have NaN value cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c74278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [flannel, wipe, opinion, worth, keep, order, s...\n",
       "1      [come, earli, disappoint, love, planet, wise, ...\n",
       "2      [soft, comfort, warmer, look, size, perfectli,...\n",
       "3      [product, worth, purchas, like, posit, ingeni,...\n",
       "4      [kid, cri, stop, tri, ween, pacifi, thumbuddi,...\n",
       "                             ...                        \n",
       "408    [sweater, nice, ferret, mind, fabric, great, p...\n",
       "409    [month, short, round, find, difficult, pant, p...\n",
       "410    [cute, hilari, recomend, good, ador, halloween...\n",
       "411    [week, look, handsom, littl, sweater, hard, sw...\n",
       "412    [want, dress, guinea, sweater, small, especi, ...\n",
       "Name: review, Length: 413, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['review'].map(preprocess)\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f595bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blue\n",
      "1 boyfor\n",
      "2 cloth\n",
      "3 countwhich\n",
      "4 face\n",
      "5 flannel\n",
      "6 hand\n",
      "7 handl\n",
      "8 higher\n",
      "9 issu\n",
      "10 keep\n"
     ]
    }
   ],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)\n",
    "count=0\n",
    "\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48fa9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79cbb83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 1),\n",
       " (24, 2),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 2),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b2634de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 13 (\"love\") appears 1 time.\n",
      "Word 24 (\"book\") appears 2 time.\n",
      "Word 25 (\"easi\") appears 1 time.\n",
      "Word 26 (\"expect\") appears 1 time.\n",
      "Word 27 (\"gift\") appears 1 time.\n",
      "Word 28 (\"go\") appears 1 time.\n",
      "Word 29 (\"great\") appears 1 time.\n",
      "Word 30 (\"help\") appears 1 time.\n",
      "Word 31 (\"kid\") appears 2 time.\n",
      "Word 32 (\"parent\") appears 1 time.\n",
      "Word 33 (\"tri\") appears 1 time.\n",
      "Word 34 (\"work\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4=bow_corpus[4]\n",
    "for i in range(len(bow_doc_4)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4[i][0], \n",
    "                                               dictionary[bow_doc_4[i][0]], \n",
    "bow_doc_4[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "391fcce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.108*\"babi\" + 0.035*\"great\" + 0.034*\"love\" + 0.027*\"time\" + 0.026*\"product\" + 0.026*\"month\" + 0.023*\"buy\" + 0.023*\"want\" + 0.018*\"like\" + 0.017*\"book\"\n",
      "Topic: 1 Word: 0.057*\"sling\" + 0.047*\"monitor\" + 0.044*\"babi\" + 0.043*\"like\" + 0.031*\"month\" + 0.028*\"great\" + 0.023*\"easi\" + 0.019*\"good\" + 0.018*\"nice\" + 0.018*\"want\"\n",
      "Topic: 2 Word: 0.046*\"love\" + 0.045*\"like\" + 0.045*\"month\" + 0.040*\"year\" + 0.035*\"babi\" + 0.034*\"look\" + 0.026*\"book\" + 0.025*\"littl\" + 0.024*\"buy\" + 0.021*\"daughter\"\n",
      "Topic: 3 Word: 0.060*\"play\" + 0.041*\"great\" + 0.031*\"like\" + 0.027*\"camera\" + 0.027*\"love\" + 0.027*\"littl\" + 0.024*\"go\" + 0.024*\"turn\" + 0.024*\"help\" + 0.022*\"product\"\n",
      "Topic: 4 Word: 0.066*\"love\" + 0.061*\"babi\" + 0.029*\"need\" + 0.026*\"camera\" + 0.025*\"easi\" + 0.022*\"recommend\" + 0.021*\"monitor\" + 0.020*\"month\" + 0.019*\"year\" + 0.019*\"littl\"\n",
      "Topic: 5 Word: 0.076*\"book\" + 0.044*\"love\" + 0.039*\"qualiti\" + 0.038*\"babi\" + 0.036*\"play\" + 0.032*\"time\" + 0.030*\"like\" + 0.029*\"cute\" + 0.027*\"great\" + 0.025*\"color\"\n",
      "Topic: 6 Word: 0.056*\"book\" + 0.042*\"month\" + 0.038*\"diaper\" + 0.035*\"love\" + 0.035*\"babi\" + 0.033*\"think\" + 0.025*\"soft\" + 0.022*\"perfect\" + 0.022*\"daughter\" + 0.019*\"start\"\n",
      "Topic: 7 Word: 0.073*\"diaper\" + 0.041*\"daughter\" + 0.038*\"great\" + 0.037*\"love\" + 0.029*\"hold\" + 0.028*\"teeth\" + 0.027*\"vibrat\" + 0.026*\"recommend\" + 0.024*\"hard\" + 0.024*\"teether\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_bow=gensim.models.LdaMulticore(bow_corpus,num_topics=8,id2word=dictionary,passes=2,workers=4)\n",
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "      print('Topic: {} Word: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab5ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.5985352396965027\t \n",
      "Topic:0.076*\"book\" + 0.044*\"love\" + 0.039*\"qualiti\" + 0.038*\"babi\" + 0.036*\"play\" + 0.032*\"time\" + 0.030*\"like\" + 0.029*\"cute\" + 0.027*\"great\" + 0.025*\"color\"\n",
      "\n",
      "Score:0.351362407207489\t \n",
      "Topic:0.060*\"play\" + 0.041*\"great\" + 0.031*\"like\" + 0.027*\"camera\" + 0.027*\"love\" + 0.027*\"littl\" + 0.024*\"go\" + 0.024*\"turn\" + 0.024*\"help\" + 0.022*\"product\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_bow[bow_corpus[4]],key=lambda tup:-1*tup[1]):\n",
    "  print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lda_model_bow.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12deb583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.5620085597038269\t \n",
      "Topic:0.076*\"book\" + 0.044*\"love\" + 0.039*\"qualiti\" + 0.038*\"babi\" + 0.036*\"play\"\n",
      "\n",
      "\n",
      "Score:0.06266074627637863\t \n",
      "Topic:0.060*\"play\" + 0.041*\"great\" + 0.031*\"like\" + 0.027*\"camera\" + 0.027*\"love\"\n",
      "\n",
      "\n",
      "Score:0.06260636448860168\t \n",
      "Topic:0.108*\"babi\" + 0.035*\"great\" + 0.034*\"love\" + 0.027*\"time\" + 0.026*\"product\"\n",
      "\n",
      "\n",
      "Score:0.06259354203939438\t \n",
      "Topic:0.056*\"book\" + 0.042*\"month\" + 0.038*\"diaper\" + 0.035*\"love\" + 0.035*\"babi\"\n",
      "\n",
      "\n",
      "Score:0.06255313009023666\t \n",
      "Topic:0.046*\"love\" + 0.045*\"like\" + 0.045*\"month\" + 0.040*\"year\" + 0.035*\"babi\"\n",
      "\n",
      "\n",
      "Score:0.0625527873635292\t \n",
      "Topic:0.073*\"diaper\" + 0.041*\"daughter\" + 0.038*\"great\" + 0.037*\"love\" + 0.029*\"hold\"\n",
      "\n",
      "\n",
      "Score:0.06251531839370728\t \n",
      "Topic:0.057*\"sling\" + 0.047*\"monitor\" + 0.044*\"babi\" + 0.043*\"like\" + 0.031*\"month\"\n",
      "\n",
      "\n",
      "Score:0.06250954419374466\t \n",
      "Topic:0.066*\"love\" + 0.061*\"babi\" + 0.029*\"need\" + 0.026*\"camera\" + 0.025*\"easi\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'This wallpaper has always been one of my favorite  '\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_bow[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    \n",
    "        print(\"\\nScore:{}\\t \\nTopic:{}\\n\".format(score,lda_model_bow.print_topic(index,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f2c5027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[flannel, wipe, opinion, worth, keep, order, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[come, earli, disappoint, love, planet, wise, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[soft, comfort, warmer, look, size, perfectli,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9269</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[product, worth, purchas, like, posit, ingeni,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>book, love, qualiti, babi, play, time, like, c...</td>\n",
       "      <td>[kid, cri, stop, tri, ween, pacifi, thumbuddi,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>408</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[sweater, nice, ferret, mind, fabric, great, p...</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[month, short, round, find, difficult, pant, p...</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>book, love, qualiti, babi, play, time, like, c...</td>\n",
       "      <td>[cute, hilari, recomend, good, ador, halloween...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5795</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[week, look, handsom, littl, sweater, hard, sw...</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>sling, monitor, babi, like, month, great, easi...</td>\n",
       "      <td>[want, dress, guinea, sweater, small, especi, ...</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0              0             4.0              0.9203   \n",
       "1              1             4.0              0.8540   \n",
       "2              2             6.0              0.6823   \n",
       "3              3             4.0              0.9269   \n",
       "4              4             5.0              0.5985   \n",
       "..           ...             ...                 ...   \n",
       "408          408             6.0              0.9026   \n",
       "409          409             6.0              0.9203   \n",
       "410          410             5.0              0.8247   \n",
       "411          411             6.0              0.5795   \n",
       "412          412             1.0              0.8247   \n",
       "\n",
       "                                              Keywords  \\\n",
       "0    love, babi, need, camera, easi, recommend, mon...   \n",
       "1    love, babi, need, camera, easi, recommend, mon...   \n",
       "2    book, month, diaper, love, babi, think, soft, ...   \n",
       "3    love, babi, need, camera, easi, recommend, mon...   \n",
       "4    book, love, qualiti, babi, play, time, like, c...   \n",
       "..                                                 ...   \n",
       "408  book, month, diaper, love, babi, think, soft, ...   \n",
       "409  book, month, diaper, love, babi, think, soft, ...   \n",
       "410  book, love, qualiti, babi, play, time, like, c...   \n",
       "411  book, month, diaper, love, babi, think, soft, ...   \n",
       "412  sling, monitor, babi, like, month, great, easi...   \n",
       "\n",
       "                                                  Text  index  \n",
       "0    [flannel, wipe, opinion, worth, keep, order, s...      0  \n",
       "1    [come, earli, disappoint, love, planet, wise, ...      1  \n",
       "2    [soft, comfort, warmer, look, size, perfectli,...      2  \n",
       "3    [product, worth, purchas, like, posit, ingeni,...      3  \n",
       "4    [kid, cri, stop, tri, ween, pacifi, thumbuddi,...      4  \n",
       "..                                                 ...    ...  \n",
       "408  [sweater, nice, ferret, mind, fabric, great, p...    408  \n",
       "409  [month, short, round, find, difficult, pant, p...    409  \n",
       "410  [cute, hilari, recomend, good, ador, halloween...    410  \n",
       "411  [week, look, handsom, littl, sweater, hard, sw...    411  \n",
       "412  [want, dress, guinea, sweater, small, especi, ...    412  \n",
       "\n",
       "[413 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topic_sentences(lda_model=lda_model_bow, corpus=bow_corpus, texts=processed_docs):\n",
    "    sent_topics_df=pd.DataFrame()\n",
    "    \n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        row=row_list[0] if lda_model.per_word_topics else row_list\n",
    "        row=sorted(row,key=lambda x:(x[1]),reverse=True)\n",
    "        \n",
    "        for j, (topic_num,prop_topic) in enumerate(row):\n",
    "            if j==0:\n",
    "                wp=lda_model.show_topic(topic_num)\n",
    "                topic_keywords=', '.join([word for word, prop in wp])\n",
    "                sent_topics_df=sent_topics_df.append(pd.Series([int(topic_num),round(prop_topic,4),topic_keywords]),ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns=['Dominant_topic','Perc_contribution','Topic_keywords']\n",
    "    contents=pd.Series(texts)\n",
    "    sent_topics_df=pd.concat([sent_topics_df,contents],axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topic_sentences(lda_model=lda_model_bow, corpus=bow_corpus, texts=processed_docs)\n",
    "\n",
    "# Format.  \n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic['index']=df_dominant_topic.index\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4584e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>index</th>\n",
       "      <th>Category</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[flannel, wipe, opinion, worth, keep, order, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[come, earli, disappoint, love, planet, wise, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Diapering</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[soft, comfort, warmer, look, size, perfectli,...</td>\n",
       "      <td>2</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9269</td>\n",
       "      <td>love, babi, need, camera, easi, recommend, mon...</td>\n",
       "      <td>[product, worth, purchas, like, posit, ingeni,...</td>\n",
       "      <td>3</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>book, love, qualiti, babi, play, time, like, c...</td>\n",
       "      <td>[kid, cri, stop, tri, ween, pacifi, thumbuddi,...</td>\n",
       "      <td>4</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>408</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[sweater, nice, ferret, mind, fabric, great, p...</td>\n",
       "      <td>408</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>The sweaters are very nice and fit quite well....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[month, short, round, find, difficult, pant, p...</td>\n",
       "      <td>409</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>My 13 month old is short and round lol. So I’m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>book, love, qualiti, babi, play, time, like, c...</td>\n",
       "      <td>[cute, hilari, recomend, good, ador, halloween...</td>\n",
       "      <td>410</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>This is sp cute and hilarious 100 recomend goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5795</td>\n",
       "      <td>book, month, diaper, love, babi, think, soft, ...</td>\n",
       "      <td>[week, look, handsom, littl, sweater, hard, sw...</td>\n",
       "      <td>411</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>Mr. B is only 4 weeks old and looks so handsom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>sling, monitor, babi, like, month, great, easi...</td>\n",
       "      <td>[want, dress, guinea, sweater, small, especi, ...</td>\n",
       "      <td>412</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>I wanted to dress up my guinea pig but the swe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0              0             4.0              0.9203   \n",
       "1              1             4.0              0.8540   \n",
       "2              2             6.0              0.6823   \n",
       "3              3             4.0              0.9269   \n",
       "4              4             5.0              0.5985   \n",
       "..           ...             ...                 ...   \n",
       "408          408             6.0              0.9026   \n",
       "409          409             6.0              0.9203   \n",
       "410          410             5.0              0.8247   \n",
       "411          411             6.0              0.5795   \n",
       "412          412             1.0              0.8247   \n",
       "\n",
       "                                              Keywords  \\\n",
       "0    love, babi, need, camera, easi, recommend, mon...   \n",
       "1    love, babi, need, camera, easi, recommend, mon...   \n",
       "2    book, month, diaper, love, babi, think, soft, ...   \n",
       "3    love, babi, need, camera, easi, recommend, mon...   \n",
       "4    book, love, qualiti, babi, play, time, like, c...   \n",
       "..                                                 ...   \n",
       "408  book, month, diaper, love, babi, think, soft, ...   \n",
       "409  book, month, diaper, love, babi, think, soft, ...   \n",
       "410  book, love, qualiti, babi, play, time, like, c...   \n",
       "411  book, month, diaper, love, babi, think, soft, ...   \n",
       "412  sling, monitor, babi, like, month, great, easi...   \n",
       "\n",
       "                                                  Text  index  \\\n",
       "0    [flannel, wipe, opinion, worth, keep, order, s...      0   \n",
       "1    [come, earli, disappoint, love, planet, wise, ...      1   \n",
       "2    [soft, comfort, warmer, look, size, perfectli,...      2   \n",
       "3    [product, worth, purchas, like, posit, ingeni,...      3   \n",
       "4    [kid, cri, stop, tri, ween, pacifi, thumbuddi,...      4   \n",
       "..                                                 ...    ...   \n",
       "408  [sweater, nice, ferret, mind, fabric, great, p...    408   \n",
       "409  [month, short, round, find, difficult, pant, p...    409   \n",
       "410  [cute, hilari, recomend, good, ador, halloween...    410   \n",
       "411  [week, look, handsom, littl, sweater, hard, sw...    411   \n",
       "412  [want, dress, guinea, sweater, small, especi, ...    412   \n",
       "\n",
       "                Category                                             review  \n",
       "0              Baby_Care  These flannel wipes are OK, but in my opinion ...  \n",
       "1              Diapering  it came early and was not disappointed. i love...  \n",
       "2                Nursery  Very soft and comfortable and warmer than it l...  \n",
       "3              Baby_Care  This is a product well worth the purchase.  I ...  \n",
       "4              Baby_Care  All of my kids have cried non-stop when I trie...  \n",
       "..                   ...                                                ...  \n",
       "408  Apparel_accessories  The sweaters are very nice and fit quite well....  \n",
       "409  Apparel_accessories  My 13 month old is short and round lol. So I’m...  \n",
       "410  Apparel_accessories  This is sp cute and hilarious 100 recomend goo...  \n",
       "411  Apparel_accessories  Mr. B is only 4 weeks old and looks so handsom...  \n",
       "412  Apparel_accessories  I wanted to dress up my guinea pig but the swe...  \n",
       "\n",
       "[413 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_1 = pd.merge(df_dominant_topic, documents[['Category','index','review']], on='index')\n",
    "\n",
    "new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "394db782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "count_vec=CountVectorizer()\n",
    "bow=count_vec.fit_transform(new_df_1['Keywords'].astype(str))\n",
    "bow=np.array(bow.todense())\n",
    "bow1=count_vec.fit_transform(new_df_1['Text'].astype(str))\n",
    "bow1=np.array(bow1.todense())\n",
    "bow2=np.concatenate((bow,bow1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb07f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=bow2\n",
    "y=new_df_1['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6021724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da35a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c53f7afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4.225220680236816 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=10000)#0.78\n",
    "# mlp = MLPClassifier(max_iter=400)\n",
    "model = MLPClassifier(max_iter=10000)\n",
    "\n",
    "# parameter_space = {\n",
    "#     'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive'],\n",
    "# }\n",
    "# model = MLPClassifier(hidden_layer_sizes=(150,100,50),\n",
    "#                         max_iter = 300,activation = 'relu',\n",
    "#                         solver = 'adam')\n",
    "\n",
    "parameter_space={\n",
    "    'hidden_layer_sizes':[(20,)],\n",
    "    'activation':['tanh'],\n",
    "    'solver':['adam'],\n",
    "    'alpha':[0.0001],\n",
    "    'learning_rate':['adaptive']\n",
    "}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(model, parameter_space, n_jobs=-1, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "# model=svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "history=model.fit(X_train,y_train)\n",
    "history\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# print('Sum of first 1 million numbers is:', sum_x)\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0d864d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6867469879518072"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1=model.predict(X_test)\n",
    "\n",
    "x11=accuracy_score(y_test,y_pred1)\n",
    "x11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0822ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "Activity_entertainment       0.67      0.29      0.40         7\n",
      "   Apparel_accessories       1.00      0.75      0.86         4\n",
      "             Baby_Care       0.75      0.88      0.81        17\n",
      "       Baby_stationary       0.53      0.73      0.62        11\n",
      "     Baby_toddler_toys       0.63      0.71      0.67        17\n",
      "             Diapering       1.00      1.00      1.00         6\n",
      "                  Gift       0.71      0.56      0.63         9\n",
      "               Nursery       0.60      0.50      0.55        12\n",
      "\n",
      "              accuracy                           0.69        83\n",
      "             macro avg       0.74      0.68      0.69        83\n",
      "          weighted avg       0.69      0.69      0.68        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4edf685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Activity_entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Gift</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>Activity_entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Nursery</td>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Nursery</td>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Gift</td>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>Activity_entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>Nursery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>Gift</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Nursery</td>\n",
       "      <td>Baby_toddler_toys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Gift</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Baby_stationary</td>\n",
       "      <td>Apparel_accessories</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Activity_entertainment</td>\n",
       "      <td>Gift</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  predicted                  actual  result\n",
       "389         Baby_stationary  Activity_entertainment       0\n",
       "117         Baby_stationary                 Nursery       0\n",
       "53                Baby_Care         Baby_stationary       0\n",
       "226               Baby_Care       Baby_toddler_toys       0\n",
       "104         Baby_stationary                    Gift       0\n",
       "396       Baby_toddler_toys  Activity_entertainment       0\n",
       "92        Baby_toddler_toys                 Nursery       0\n",
       "197                 Nursery       Baby_toddler_toys       0\n",
       "113                 Nursery       Baby_toddler_toys       0\n",
       "63                     Gift         Baby_stationary       0\n",
       "202               Baby_Care       Baby_toddler_toys       0\n",
       "119         Baby_stationary                 Nursery       0\n",
       "381               Baby_Care  Activity_entertainment       0\n",
       "178       Baby_toddler_toys                 Nursery       0\n",
       "231       Baby_toddler_toys         Baby_stationary       0\n",
       "58        Baby_toddler_toys                    Gift       0\n",
       "216                 Nursery       Baby_toddler_toys       0\n",
       "326         Baby_stationary                    Gift       0\n",
       "412         Baby_stationary     Apparel_accessories       0\n",
       "191  Activity_entertainment                    Gift       0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(data={'predicted': y_pred1, 'actual': y_test})\n",
    "results['result'] = np.where(results['predicted']==results['actual'], 1, 0)\n",
    "results.sort_values(by='result').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3939bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plot_confusion_matrix(model, X_train, y_train, display_labels=model.classes_)\n",
    "# fig.figure_.suptitle(\"Confusion Matrix for the model\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5eb178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(model.loss_curve_)\n",
    "# # plt.plot(model.coef_)\n",
    "# plt.title(\"Loss Curve\", fontsize=14)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Cost')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50f30164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Best parameters found:\\n', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf1c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9a95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd4926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
